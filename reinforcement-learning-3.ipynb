{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "    def seed(self, env):\n",
    "        i = 0\n",
    "        while i < self.max_size:\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while True:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = np.zeros(state.shape) if done else next_state\n",
    "                self.add((state, action, reward, next_state))\n",
    "                if done:\n",
    "                    break\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAgent(object):\n",
    "    def __init__(self, state_shape, num_actions, learning_rate=1e-3, hidden_size=32, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.action = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.actions_one_hot = tf.one_hot(self.action, num_actions)\n",
    "        \n",
    "        self.state = tf.placeholder(tf.float32, shape=[None, *state_shape])\n",
    "        # self.layer0 = tf.layers.dense(self.state, hidden_size, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(stddev=1e-1))\n",
    "        # self.layer1 = tf.layers.dense(self.layer0, hidden_size, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(stddev=1e-1))\n",
    "        # self.value = tf.layers.dense(self.layer1, num_actions, activation=None)\n",
    "        self.layer0 = tf.contrib.layers.fully_connected(self.state, hidden_size)\n",
    "        self.layer1 = tf.contrib.layers.fully_connected(self.layer0, hidden_size)\n",
    "        self.value = tf.contrib.layers.fully_connected(self.layer1, num_actions, activation_fn=None)\n",
    "\n",
    "        self.best_values, self.best_actions = tf.nn.top_k(self.value, k=2)\n",
    "        self.best_action = tf.squeeze(self.best_actions[:, 0])\n",
    "        self.second_best_action = tf.squeeze(self.best_actions[:, 1])\n",
    "        self.best_reward = tf.squeeze(self.best_values[:, 0])\n",
    "        self.second_best_reward = tf.squeeze(self.best_values[:, 1])\n",
    "#         self.best_action = tf.squeeze(tf.argmax(self.value, axis=1))\n",
    "#         self.best_reward = tf.squeeze(tf.reduce_max(self.value, axis=1))\n",
    "\n",
    "        # self.expected_reward = tf.squeeze(tf.gather(self.value, self.action, axis=1))\n",
    "        self.expected_reward = tf.reduce_sum(tf.multiply(self.value, self.actions_one_hot), axis=1)\n",
    "        \n",
    "        # self.reward = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.target = tf.placeholder(tf.float32, shape=[None])\n",
    "        # self.next_reward = tf.placeholder(tf.float32, shape=[None])\n",
    "        # self.loss = tf.nn.l2_loss(self.reward + self.gamma * self.next_reward - self.expected_reward)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.target - self.expected_reward)) + tf.reduce_mean(tf.square(self.best_reward - self.second_best_reward))\n",
    "        self.train = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "def train(sess, env, agent, num_episodes, explore_decay=1e-4, batch_size=20, buffer_size=10000):\n",
    "    memory = Memory(max_size=buffer_size)\n",
    "    memory.seed(env)\n",
    "    for i in range(num_episodes):\n",
    "        epsilon = np.exp(-explore_decay*i)\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                value = None\n",
    "            else:\n",
    "                action, value = sess.run([agent.best_action, agent.value], feed_dict={\n",
    "                    agent.state: [state],\n",
    "                })\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # if value is not None:\n",
    "                # print(state, value, action, reward)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                # next_state is 0\n",
    "                # print(state, reward)\n",
    "                memory.add((state, action, reward, np.zeros(state.shape)))\n",
    "                break\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            next_state = state\n",
    "            # run training\n",
    "            loss = train_step(sess, agent, memory, batch_size)\n",
    "\n",
    "        if i % (num_episodes//30) == 0:\n",
    "            print(i, loss, epsilon, total_reward)\n",
    "\n",
    "def train_step(sess, agent, memory, batch_size):\n",
    "    state, action, reward, next_state = zip(*memory.sample(batch_size))\n",
    "    next_reward, = sess.run([agent.best_reward], feed_dict={\n",
    "        agent.state: next_state,\n",
    "    })\n",
    "    episode_ends = (next_state == np.zeros(state[0].shape)).all(axis=1)\n",
    "    target = reward + agent.gamma * next_reward\n",
    "    target[episode_ends] = 0\n",
    "    _, loss = sess.run([agent.train, agent.loss], feed_dict={\n",
    "        agent.state: state,\n",
    "        agent.action: action,\n",
    "        agent.target: target,\n",
    "    })\n",
    "    return loss\n",
    "\n",
    "def play(sess, env, agent):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, value = sess.run([agent.best_action, agent.value], feed_dict={\n",
    "            agent.state: [state],\n",
    "        })\n",
    "        state, reward, done, info = env.step(action)\n",
    "        print(state, value, action, reward)\n",
    "        env.render()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "0 1.0145406 1.0 12.0\n",
      "33 6.6106834 0.9967054390154381 16.0\n",
      "66 80.244316 0.9934217321629571 26.0\n",
      "99 76.3348 0.9901488436829572 26.0\n",
      "132 20.144272 0.9868867379336502 12.0\n",
      "165 214.69324 0.9836353793906724 16.0\n",
      "198 316.42194 0.9803947326466971 46.0\n",
      "231 119.69615 0.9771647624110493 12.0\n",
      "264 144.49698 0.9739454335093212 14.0\n",
      "297 155.05605 0.9707367108829891 15.0\n",
      "330 11.88345 0.967538559589032 9.0\n",
      "363 146.74892 0.9643509447995507 77.0\n",
      "396 273.38467 0.9611738318013887 35.0\n",
      "429 23.965414 0.958007185995754 16.0\n",
      "462 32.716953 0.9548509728978424 17.0\n",
      "495 301.8011 0.9517051581364622 24.0\n",
      "528 64.54038 0.9485697074536594 21.0\n",
      "561 316.02863 0.9454445867043453 15.0\n",
      "594 15.578945 0.9423297618559239 15.0\n",
      "627 24.464447 0.9392251989879219 21.0\n",
      "660 134.21796 0.9361308642916188 52.0\n",
      "693 356.30508 0.9330467240696795 56.0\n",
      "726 347.13812 0.9299727447357862 12.0\n",
      "759 116.21411 0.9269088928142737 21.0\n",
      "792 28.124771 0.9238551349397642 25.0\n",
      "825 20.413017 0.9208114378568045 33.0\n",
      "858 211.068 0.9177777684195032 31.0\n",
      "891 15.939557 0.91475409359117 10.0\n",
      "924 217.21056 0.9117403804439562 28.0\n",
      "957 130.56017 0.9087365961584959 14.0\n",
      "990 21.966772 0.9057427080235485 17.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "tf.reset_default_graph()\n",
    "agent = NeuralAgent(env.reset().shape, env.action_space.n, learning_rate=1e-4, hidden_size=64, gamma=0.999)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train(sess, env, agent, 1000, explore_decay=1e-4, batch_size=20, buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 0.02698689  0.21116858  0.03766834 -0.23525833] [[48.649868 48.749046]] 1 1.0\n",
      "[ 0.03121026  0.40573266  0.03296318 -0.51582555] [[48.751907 48.946613]] 1 1.0\n",
      "[ 0.03932491  0.6003753   0.02264666 -0.79794149] [[48.667076 48.909157]] 1 1.0\n",
      "[ 0.05133242  0.79517934  0.00668783 -1.08341506] [[49.053036 49.32095 ]] 1 1.0\n",
      "[ 0.067236    0.9902124  -0.01498047 -1.3739919 ] [[50.333035 50.608376]] 1 1.0\n",
      "[ 0.08704025  1.18551836 -0.0424603  -1.67132207] [[52.854507 53.14676 ]] 1 1.0\n",
      "[ 0.11075062  1.38110706 -0.07588675 -1.97692027] [[55.74008  56.066074]] 1 1.0\n",
      "[ 0.13837276  1.57694173 -0.11542515 -2.29211615] [[58.87269  59.244522]] 1 1.0\n",
      "[ 0.16991159  1.77292386 -0.16126747 -2.61799291] [[62.08731  62.505493]] 1 1.0\n",
      "[ 0.20537007  1.96887538 -0.21362733 -2.95531375] [[65.77322  66.221344]] 1 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "play(sess, env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_play(env):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = int(input())\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "2 (<class 'int'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-aa6d5e6da16e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuman_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-164-edc46734349e>\u001b[0m in \u001b[0;36mhuman_play\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 2 (<class 'int'>) invalid"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "human_play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, num_actions, state_encoder=None, gamma=1, alpha=1e-1):\n",
    "        # values is a dictionary mapping state -> estimated reward for each action\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.state_encoder = state_encoder\n",
    "        self.values = defaultdict(lambda: np.zeros(num_actions))\n",
    "    \n",
    "    def choose_action(self, state, epsilon=0):\n",
    "        '''\n",
    "        Chooses an action according to an epsilon-greedy strategy.\n",
    "        epsilon=0 corresponds to a pure greedy strategy\n",
    "        epsilon=1 corresponds to a pure random strategy\n",
    "        \n",
    "        Arguments:\n",
    "            state: a structure representing the environments state\n",
    "            epsilon: a number between 0 and 1 inclusive\n",
    "        \n",
    "        Returns:\n",
    "            action: an integer representing the action\n",
    "            reward: the predicted reward\n",
    "        '''\n",
    "        if self.state_encoder is not None:\n",
    "            state = self.state_encoder.encode_state(state)\n",
    "        if state in self.values:\n",
    "            best = np.argmax(self.values[state])\n",
    "            if epsilon == 0:\n",
    "                return best, self.values[state][best]\n",
    "            probs = np.ones(self.num_actions, dtype=np.float32) * epsilon / self.num_actions\n",
    "            probs[best] += 1 - epsilon\n",
    "        else:\n",
    "            probs = np.ones(self.num_actions, dtype=np.float32) / float(self.num_actions)\n",
    "        action = np.random.choice(self.num_actions, p=probs)\n",
    "        return action, self.values[state][action]\n",
    "    \n",
    "    def step(self, state, action, reward, next_state):\n",
    "        if self.state_encoder is not None:\n",
    "            state = self.state_encoder.encode_state(state)\n",
    "            next_state = self.state_encoder.encode_state(next_state)\n",
    "        next_action, _ = self.choose_action(next_state, epsilon=0)\n",
    "        true_value = reward + self.gamma * self.values[next_state][next_action]\n",
    "        error = true_value - self.values[state][action]\n",
    "        self.values[state][action] += self.alpha * error\n",
    "        return error\n",
    "\n",
    "class Discretizer(object):\n",
    "    def __init__(self, env, num_buckets, num_sample_episodes=1000):\n",
    "        self.env = env\n",
    "        samples = []\n",
    "        for i in range(num_sample_episodes):\n",
    "            state = env.reset()\n",
    "            samples.append(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                state, reward, done, info = env.step(np.random.choice(env.action_space.n))\n",
    "                samples.append(state)\n",
    "        samples = np.array(samples)\n",
    "        self.low = np.min(samples, axis=0)\n",
    "        self.high = np.max(samples, axis=0)\n",
    "        self.range = self.high - self.low\n",
    "        self.n = num_buckets\n",
    "\n",
    "    def encode_state(self, state):\n",
    "        '''Encode state takes in an environments state and returns a tuple.'''\n",
    "        d = np.round((state - self.low) / self.range * self.n).astype(np.int32)\n",
    "        return tuple(np.clip(d, 0, self.n))\n",
    "    \n",
    "\n",
    "def train(env, agent, num_episodes):\n",
    "    batch_error = []\n",
    "    batch_steps = []\n",
    "    batch_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        epsilon = np.exp((-1e-4)*i)\n",
    "        state = env.reset()\n",
    "        errors = list()\n",
    "        steps = 1\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action, pred_reward = agent.choose_action(state, epsilon=epsilon)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward * 10 - 1\n",
    "            error = agent.step(state, action, reward, next_state)\n",
    "#             errors.append(error**2)\n",
    "            errors.append(error)\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        batch_error.append(np.mean(errors))\n",
    "        batch_steps.append(steps)\n",
    "        batch_rewards.append(total_reward)\n",
    "        if i % 100 == 0:\n",
    "            print(\"Error: %.2f\\tReward: %.2f\\tSteps: %d\" % (np.mean(batch_error), np.mean(batch_rewards), np.mean(batch_steps)))\n",
    "            batch_error = []\n",
    "            batch_steps = []\n",
    "            batch_rewards = []\n",
    "\n",
    "def play(env, agent):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    errors = []\n",
    "    rewards = []\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        # use greedy strategy\n",
    "        action, pred_reward = agent.choose_action(state, epsilon=0)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        errors.append(reward - pred_reward)\n",
    "        rewards.append(reward)\n",
    "        steps += 1\n",
    "        env.render()\n",
    "    env.close()\n",
    "    return sum(rewards), steps, np.mean(np.array(errors)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
