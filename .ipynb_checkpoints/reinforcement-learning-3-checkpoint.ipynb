{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "    def seed(self, env):\n",
    "        i = 0\n",
    "        while i < self.max_size:\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while True:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = np.zeros(state.shape) if done else next_state\n",
    "                self.add((state, action, reward, next_state))\n",
    "                if done:\n",
    "                    break\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAgent(object):\n",
    "    def __init__(self, state_shape, num_actions, learning_rate=1e-3, hidden_size=32, gamma=0.9, top_two=100):\n",
    "        self.gamma = gamma\n",
    "        self.top_two = tf.constant(top_two, dtype=tf.float32)\n",
    "        \n",
    "        self.action = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.actions_one_hot = tf.one_hot(self.action, num_actions)\n",
    "        \n",
    "        self.state = tf.placeholder(tf.float32, shape=[None, *state_shape])\n",
    "        # self.layer0 = tf.layers.dense(self.state, hidden_size, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(stddev=1e-1))\n",
    "        # self.layer1 = tf.layers.dense(self.layer0, hidden_size, activation=tf.nn.relu, kernel_initializer=tf.truncated_normal_initializer(stddev=1e-1))\n",
    "        # self.value = tf.layers.dense(self.layer1, num_actions, activation=None)\n",
    "        self.layer0 = tf.contrib.layers.fully_connected(self.state, hidden_size)\n",
    "        self.layer1 = tf.contrib.layers.fully_connected(self.layer0, hidden_size)\n",
    "        self.value = tf.contrib.layers.fully_connected(self.layer1, num_actions, activation_fn=None)\n",
    "\n",
    "        self.best_values, self.best_actions = tf.nn.top_k(self.value, k=2)\n",
    "        self.best_action = tf.squeeze(self.best_actions[:, 0])\n",
    "        self.second_best_action = tf.squeeze(self.best_actions[:, 1])\n",
    "        self.best_reward = tf.squeeze(self.best_values[:, 0])\n",
    "        self.second_best_reward = tf.squeeze(self.best_values[:, 1])\n",
    "#         self.best_action = tf.squeeze(tf.argmax(self.value, axis=1))\n",
    "#         self.best_reward = tf.squeeze(tf.reduce_max(self.value, axis=1))\n",
    "\n",
    "        # self.expected_reward = tf.squeeze(tf.gather(self.value, self.action, axis=1))\n",
    "        self.expected_reward = tf.reduce_sum(tf.multiply(self.value, self.actions_one_hot), axis=1)\n",
    "        \n",
    "        # self.reward = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.target = tf.placeholder(tf.float32, shape=[None])\n",
    "        # self.next_reward = tf.placeholder(tf.float32, shape=[None])\n",
    "        # self.loss = tf.nn.l2_loss(self.reward + self.gamma * self.next_reward - self.expected_reward)\n",
    "        best_actions_float = tf.cast(self.best_actions, tf.float32)\n",
    "        self.print = tf.Print([self.value, self.best_values, best_actions_float], [self.value, self.best_values, best_actions_float])\n",
    "        self.loss = tf.reduce_mean(tf.square(self.target - self.expected_reward)) + self.top_two*tf.reduce_mean(tf.square(self.best_reward - self.second_best_reward))\n",
    "        self.train = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "def train(sess, env, agent, num_episodes, explore_decay=1e-4, batch_size=20, buffer_size=10000):\n",
    "    memory = Memory(max_size=buffer_size)\n",
    "    memory.seed(env)\n",
    "    for i in range(num_episodes):\n",
    "        epsilon = np.exp(-explore_decay*i)\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                value = None\n",
    "            else:\n",
    "                action, value = sess.run([agent.best_action, agent.value], feed_dict={\n",
    "                    agent.state: [state],\n",
    "                })\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # if value is not None:\n",
    "                # print(state, value, action, reward)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                # next_state is 0\n",
    "                # print(state, reward)\n",
    "                memory.add((state, action, reward, np.zeros(state.shape)))\n",
    "                break\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            next_state = state\n",
    "            # run training\n",
    "            loss = train_step(sess, agent, memory, batch_size)\n",
    "\n",
    "        if i % (num_episodes//30) == 0:\n",
    "            print(i, loss, epsilon, total_reward)\n",
    "\n",
    "def train_step(sess, agent, memory, batch_size):\n",
    "    state, action, reward, next_state = zip(*memory.sample(batch_size))\n",
    "    next_reward, = sess.run([agent.best_reward], feed_dict={\n",
    "        agent.state: next_state,\n",
    "    })\n",
    "    episode_ends = (next_state == np.zeros(state[0].shape)).all(axis=1)\n",
    "    target = reward + agent.gamma * next_reward\n",
    "    target[episode_ends] = -10\n",
    "    _, loss = sess.run([\n",
    "        agent.train,\n",
    "        agent.loss,\n",
    "    #    agent.print\n",
    "    ], feed_dict={\n",
    "        agent.state: state,\n",
    "        agent.action: action,\n",
    "        agent.target: target,\n",
    "    })\n",
    "    return loss\n",
    "\n",
    "def play(sess, env, agent):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, value = sess.run([agent.best_action, agent.value], feed_dict={\n",
    "            agent.state: [state],\n",
    "        })\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        print(state, value, action, reward)\n",
    "        env.render()\n",
    "    env.close()\n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "0 203.0741 1.0 26.0\n",
      "66 3.306761 0.9361308642916188 9.0\n",
      "132 6.7076674 0.8763409950793732 29.0\n",
      "198 16.291662 0.8203698531378311 15.0\n",
      "264 6.0952826 0.7679735396567061 16.0\n",
      "330 1.2085667 0.7189237334319262 12.0\n",
      "396 11.009008 0.6730066959373864 23.0\n",
      "462 1.1906924 0.6300223399419123 16.0\n",
      "528 6.4316063 0.5897833576128504 12.0\n",
      "594 1.3088791 0.5521144043069306 11.0\n",
      "660 1.3286767 0.5168513344916992 18.0\n",
      "726 11.222813 0.483840486467991 18.0\n",
      "792 6.1772714 0.4529380127765577 16.0\n",
      "858 1.1166687 0.4240092533710473 15.0\n",
      "924 11.093655 0.39692814882588245 19.0\n",
      "990 6.151258 0.3715766910220457 11.0\n",
      "1056 16.170982 0.3478444089170874 10.0\n",
      "1122 6.0968285 0.32562788715856034 10.0\n",
      "1188 6.174362 0.30483031544319683 14.0\n",
      "1254 11.08802 0.28536106665812666 12.0\n",
      "1320 6.209101 0.26713530196585034 10.0\n",
      "1386 11.166468 0.2500736011120941 9.0\n",
      "1452 1.088772 0.2341016163455822 24.0\n",
      "1518 11.09774 0.2191497484416548 9.0\n",
      "1584 6.1119013 0.20515284341797715 12.0\n",
      "1650 1.2172022 0.19204990862075408 8.0\n",
      "1716 6.132843 0.17978384694427296 11.0\n",
      "1782 11.037975 0.16830120802561438 19.0\n",
      "1848 6.1109557 0.15755195533034191 10.0\n",
      "1914 1.1143619 0.14748924811422748 17.0\n",
      "1980 6.1364293 0.13806923731089282 55.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "tf.reset_default_graph()\n",
    "agent = NeuralAgent(env.reset().shape, env.action_space.n, learning_rate=1e-4, hidden_size=64, gamma=0.999, top_two=1e7)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train(sess, env, agent, 2000, explore_decay=1e-3, batch_size=20, buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[-0.00561728 -0.22225645  0.01109633  0.3325386 ] [[0.0038058  0.00372704]] 0 1.0\n",
      "[-0.01006241 -0.02729419  0.0177471   0.04337545] [[-0.01818687 -0.00615237]] 1 1.0\n",
      "[-0.0106083   0.16756884  0.01861461 -0.24365568] [[0.00369611 0.00380344]] 1 1.0\n",
      "[-0.00725692 -0.02781398  0.0137415   0.05484015] [[0.01729968 0.01666563]] 0 1.0\n",
      "[-0.0078132  -0.22313024  0.0148383   0.35182675] [[0.00333026 0.00316543]] 0 1.0\n",
      "[-0.0122758  -0.02822241  0.02187484  0.06385949] [[-0.0192674  -0.00571072]] 1 1.0\n",
      "[-0.01284025 -0.22365105  0.02315203  0.36336301] [[0.00349962 0.00316365]] 0 1.0\n",
      "[-0.01731327 -0.02886567  0.03041929  0.07806933] [[-0.01928915 -0.00517028]] 1 1.0\n",
      "[-0.01789059 -0.22441019  0.03198067  0.38019231] [[0.00328123 0.00270816]] 0 1.0\n",
      "[-0.02237879 -0.02975663  0.03958452  0.09776198] [[-0.01967306 -0.00459427]] 1 1.0\n",
      "[-0.02297392 -0.2254229   0.04153976  0.40266627] [[0.00303065 0.0022296 ]] 0 1.0\n",
      "[-0.02748238 -0.03091398  0.04959308  0.12336376] [[-0.02044959 -0.00432236]] 1 1.0\n",
      "[-0.02810066 -0.22671006  0.05206036  0.43127118] [[0.00241761 0.00228817]] 0 1.0\n",
      "[-0.03263486 -0.03236244  0.06068578  0.15544356] [[-0.02159695 -0.00400522]] 1 1.0\n",
      "[-0.03328211  0.16184048  0.06379465 -0.11749392] [[0.00158499 0.00271523]] 1 1.0\n",
      "[-0.0300453  -0.03413476  0.06144478  0.19461387] [[0.0184764  0.01140593]] 0 1.0\n",
      "[-0.030728    0.16005689  0.06533705 -0.0780705 ] [[0.00027924 0.00311213]] 1 1.0\n",
      "[-0.02752686 -0.0359379   0.06377564  0.23448982] [[0.019554   0.01043391]] 0 1.0\n",
      "[-0.02824562  0.1582176   0.06846544 -0.03741404] [[-0.00122007  0.00325328]] 1 1.0\n",
      "[-0.02508126 -0.03781595  0.06771716  0.2760605 ] [[0.02000274 0.01006126]] 0 1.0\n",
      "[-0.02583758  0.15627781  0.07323837  0.00548076] [[-0.0029722  0.0033545]] 1 1.0\n",
      "[-0.02271203 -0.03981391  0.07334798  0.32034268] [[0.01783081 0.0119217 ]] 0 1.0\n",
      "[-0.0235083   0.154191    0.07975484  0.05166274] [[-0.00500733  0.00344968]] 1 1.0\n",
      "[-0.02042448  0.34808415  0.08078809 -0.21482941] [[0.01378556 0.01629405]] 1 1.0\n",
      "[-0.0134628   0.15190573  0.0764915   0.10220431] [[0.03050009 0.01390566]] 0 1.0\n",
      "[-0.01042469  0.34585286  0.07853559 -0.16539914] [[0.00995974 0.01578061]] 1 1.0\n",
      "[-0.00350763  0.14969966  0.07522761  0.15098948] [[0.03040501 0.01323632]] 0 1.0\n",
      "[-0.00051364  0.3436683   0.0782474  -0.11704453] [[0.00942759 0.01520295]] 1 1.0\n",
      "[0.00635973 0.14751749 0.07590651 0.1992627 ] [[0.02917193 0.01382558]] 0 1.0\n",
      "[ 0.00931008  0.34147633  0.07989176 -0.06854271] [[0.00779231 0.01300898]] 1 1.0\n",
      "[0.01613961 0.14530527 0.07852091 0.2482385 ] [[0.03047125 0.01525916]] 0 1.0\n",
      "[ 0.01904571  0.33922315  0.08348568 -0.01868065] [[0.00582061 0.01119136]] 1 1.0\n",
      "[0.02583017 0.14300931 0.08311206 0.29913131] [[0.0281763  0.01945216]] 0 1.0\n",
      "[0.02869036 0.33685429 0.08909469 0.03377434] [[0.00151929 0.00877776]] 1 1.0\n",
      "[0.03542745 0.14057514 0.08977018 0.3531845 ] [[0.02510759 0.02251305]] 0 1.0\n",
      "[0.03823895 0.33431354 0.09683387 0.09010408] [[-0.00465085  0.00575038]] 1 1.0\n",
      "[ 0.04492522  0.52792376  0.09863595 -0.17052605] [[0.01938437 0.02716131]] 1 1.0\n",
      "[0.05548369 0.33153853 0.09522543 0.15157195] [[0.0322932  0.01904352]] 0 1.0\n",
      "[ 0.06211447  0.52517709  0.09825687 -0.10961598] [[0.01070755 0.02584268]] 1 1.0\n",
      "[0.07261801 0.32879447 0.09606455 0.21237817] [[0.03060994 0.02147645]] 0 1.0\n",
      "[ 0.0791939   0.52242105  0.10031211 -0.04852302] [[0.00539999 0.02428334]] 1 1.0\n",
      "[0.08964232 0.32601447 0.09934165 0.27404816] [[0.02773692 0.02571615]] 0 1.0\n",
      "[0.09616261 0.51958901 0.10482261 0.01427635] [[-0.00024921  0.02017063]] 1 1.0\n",
      "[ 0.10655439  0.71306368  0.10510814 -0.24358128] [[0.02370501 0.02982222]] 1 1.0\n",
      "[0.12081566 0.5166097  0.10023651 0.08031834] [[0.03183366 0.02692161]] 0 1.0\n",
      "[ 0.13114786  0.71016254  0.10184288 -0.17913339] [[0.01799063 0.03324276]] 1 1.0\n",
      "[ 0.14535111  0.90369081  0.09826021 -0.43803157] [[0.02678686 0.02828415]] 1 1.0\n",
      "[ 0.16342492  0.70732536  0.08949958 -0.11606216] [[0.03718169 0.03382196]] 0 1.0\n",
      "[ 0.17757143  0.90105852  0.08717834 -0.37922105] [[0.02185994 0.030168  ]] 1 1.0\n",
      "[ 0.1955926   1.09484126  0.07959392 -0.6431933 ] [[0.03106478 0.03511756]] 1 1.0\n",
      "[ 0.21748942  1.28876892  0.06673005 -0.90978794] [[0.03837898 0.04061963]] 1 1.0\n",
      "[ 0.2432648   1.48292738  0.04853429 -1.18077337] [[0.04441192 0.04957056]] 1 1.0\n",
      "[ 0.27292335  1.67738687  0.02491883 -1.45785564] [[0.04990555 0.06643681]] 1 1.0\n",
      "[ 0.30647109  1.87219441 -0.00423829 -1.74265086] [[0.05635416 0.08696739]] 1 1.0\n",
      "[ 0.34391498  2.06736433 -0.0390913  -2.03664916] [[0.06398144 0.10823025]] 1 1.0\n",
      "[ 0.38526226  2.26286634 -0.07982429 -2.34116805] [[0.06824182 0.1315563 ]] 1 1.0\n",
      "[ 0.43051959  2.45861076 -0.12664765 -2.65729316] [[0.07117471 0.15527186]] 1 1.0\n",
      "[ 0.47969181  2.65443093 -0.17979351 -2.98580584] [[0.07274081 0.17927296]] 1 1.0\n",
      "[ 0.53278042  2.85006305 -0.23950963 -3.32709879] [[0.07547593 0.20587549]] 1 1.0\n",
      "59.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "play(sess, env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_play(env):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = int(input())\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "2 (<class 'int'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-aa6d5e6da16e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuman_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-164-edc46734349e>\u001b[0m in \u001b[0;36mhuman_play\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 2 (<class 'int'>) invalid"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "human_play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, num_actions, state_encoder=None, gamma=1, alpha=1e-1):\n",
    "        # values is a dictionary mapping state -> estimated reward for each action\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.state_encoder = state_encoder\n",
    "        self.values = defaultdict(lambda: np.zeros(num_actions))\n",
    "    \n",
    "    def choose_action(self, state, epsilon=0):\n",
    "        '''\n",
    "        Chooses an action according to an epsilon-greedy strategy.\n",
    "        epsilon=0 corresponds to a pure greedy strategy\n",
    "        epsilon=1 corresponds to a pure random strategy\n",
    "        \n",
    "        Arguments:\n",
    "            state: a structure representing the environments state\n",
    "            epsilon: a number between 0 and 1 inclusive\n",
    "        \n",
    "        Returns:\n",
    "            action: an integer representing the action\n",
    "            reward: the predicted reward\n",
    "        '''\n",
    "        if self.state_encoder is not None:\n",
    "            state = self.state_encoder.encode_state(state)\n",
    "        if state in self.values:\n",
    "            best = np.argmax(self.values[state])\n",
    "            if epsilon == 0:\n",
    "                return best, self.values[state][best]\n",
    "            probs = np.ones(self.num_actions, dtype=np.float32) * epsilon / self.num_actions\n",
    "            probs[best] += 1 - epsilon\n",
    "        else:\n",
    "            probs = np.ones(self.num_actions, dtype=np.float32) / float(self.num_actions)\n",
    "        action = np.random.choice(self.num_actions, p=probs)\n",
    "        return action, self.values[state][action]\n",
    "    \n",
    "    def step(self, state, action, reward, next_state):\n",
    "        if self.state_encoder is not None:\n",
    "            state = self.state_encoder.encode_state(state)\n",
    "            next_state = self.state_encoder.encode_state(next_state)\n",
    "        next_action, _ = self.choose_action(next_state, epsilon=0)\n",
    "        true_value = reward + self.gamma * self.values[next_state][next_action]\n",
    "        error = true_value - self.values[state][action]\n",
    "        self.values[state][action] += self.alpha * error\n",
    "        return error\n",
    "\n",
    "class Discretizer(object):\n",
    "    def __init__(self, env, num_buckets, num_sample_episodes=1000):\n",
    "        self.env = env\n",
    "        samples = []\n",
    "        for i in range(num_sample_episodes):\n",
    "            state = env.reset()\n",
    "            samples.append(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                state, reward, done, info = env.step(np.random.choice(env.action_space.n))\n",
    "                samples.append(state)\n",
    "        samples = np.array(samples)\n",
    "        self.low = np.min(samples, axis=0)\n",
    "        self.high = np.max(samples, axis=0)\n",
    "        self.range = self.high - self.low\n",
    "        self.n = num_buckets\n",
    "\n",
    "    def encode_state(self, state):\n",
    "        '''Encode state takes in an environments state and returns a tuple.'''\n",
    "        d = np.round((state - self.low) / self.range * self.n).astype(np.int32)\n",
    "        return tuple(np.clip(d, 0, self.n))\n",
    "    \n",
    "\n",
    "def train(env, agent, num_episodes):\n",
    "    batch_error = []\n",
    "    batch_steps = []\n",
    "    batch_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        epsilon = np.exp((-1e-4)*i)\n",
    "        state = env.reset()\n",
    "        errors = list()\n",
    "        steps = 1\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action, pred_reward = agent.choose_action(state, epsilon=epsilon)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward * 10 - 1\n",
    "            error = agent.step(state, action, reward, next_state)\n",
    "#             errors.append(error**2)\n",
    "            errors.append(error)\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        batch_error.append(np.mean(errors))\n",
    "        batch_steps.append(steps)\n",
    "        batch_rewards.append(total_reward)\n",
    "        if i % 100 == 0:\n",
    "            print(\"Error: %.2f\\tReward: %.2f\\tSteps: %d\" % (np.mean(batch_error), np.mean(batch_rewards), np.mean(batch_steps)))\n",
    "            batch_error = []\n",
    "            batch_steps = []\n",
    "            batch_rewards = []\n",
    "\n",
    "def play(env, agent):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    errors = []\n",
    "    rewards = []\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        # use greedy strategy\n",
    "        action, pred_reward = agent.choose_action(state, epsilon=0)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        errors.append(reward - pred_reward)\n",
    "        rewards.append(reward)\n",
    "        steps += 1\n",
    "        env.render()\n",
    "    env.close()\n",
    "    return sum(rewards), steps, np.mean(np.array(errors)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
