{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAgent(object):\n",
    "    def __init__(self, state_shape, num_actions, learning_rate=1e-3, hidden_size=32, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.action = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.actions_one_hot = tf.one_hot(self.action, num_actions)\n",
    "        \n",
    "        self.state = tf.placeholder(tf.float32, shape=[None, *state_shape])\n",
    "        self.layer0 = tf.layers.dense(self.state, hidden_size, activation=tf.nn.sigmoid)\n",
    "        self.layer1 = tf.layers.dense(self.layer0, hidden_size, activation=tf.nn.sigmoid)\n",
    "        self.value = tf.layers.dense(self.layer1, num_actions, activation=None)\n",
    "\n",
    "        self.best_action = tf.squeeze(tf.argmax(self.value, axis=1))\n",
    "        self.best_reward = tf.squeeze(tf.reduce_max(self.value, axis=1))\n",
    "\n",
    "        # self.expected_reward = tf.squeeze(tf.gather(self.value, self.action, axis=1))\n",
    "        self.expected_reward = tf.reduce_sum(tf.multiply(self.value, self.actions_one_hot), axis=1)\n",
    "        \n",
    "        # self.reward = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.target = tf.placeholder(tf.float32, shape=[None])\n",
    "        # self.next_reward = tf.placeholder(tf.float32, shape=[None])\n",
    "        # self.loss = tf.nn.l2_loss(self.reward + self.gamma * self.next_reward - self.expected_reward)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.target - self.expected_reward))\n",
    "        self.train = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "def train(sess, env, agent, num_episodes, explore_decay=1e-4):\n",
    "    for i in range(num_episodes):\n",
    "        epsilon = np.exp(-explore_decay*i)\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        while True:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action, = sess.run([agent.best_action], feed_dict={\n",
    "                    agent.state: [state],\n",
    "                })\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                # next_state is 0\n",
    "                episode.append((state, action, reward, np.zeros(state.shape)))\n",
    "                break\n",
    "            episode.append((state, action, reward, next_state))\n",
    "            next_state = state\n",
    "        state, action, reward, next_state = zip(*episode)\n",
    "        next_reward, = sess.run([agent.best_reward], feed_dict={\n",
    "            agent.state: next_state,\n",
    "        })\n",
    "        episode_ends = (next_state == np.zeros(state[0].shape)).all(axis=1)\n",
    "        target = reward + agent.gamma * next_reward\n",
    "        target[episode_ends] = 0\n",
    "        _, loss = sess.run([agent.train, agent.loss], feed_dict={\n",
    "            agent.state: state,\n",
    "            agent.action: action,\n",
    "            agent.target: target,\n",
    "            # agent.reward: reward,\n",
    "            # agent.next_reward: next_reward,\n",
    "        })\n",
    "        if i % (num_episodes//30) == 0:\n",
    "            print(i, loss, epsilon)\n",
    "        \n",
    "def play(sess, env, agent):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, = sess.run([agent.best_action], feed_dict={\n",
    "            agent.state: [state],\n",
    "        })\n",
    "        state, reward, done, info = env.step(action)\n",
    "        print(state, action, reward)\n",
    "        env.render()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "0 9.737889 1.0\n",
      "333 5.8244243 0.9672483415560369\n",
      "666 11.266284 0.9355693542429038\n",
      "999 6.285566 0.9049279063021011\n",
      "1332 19.181429 0.8752900165984839\n",
      "1665 7.7142377 0.8466228169354396\n",
      "1998 15.550501 0.8188945156043043\n",
      "2331 12.143339 0.7920743621275974\n",
      "2664 6.5326624 0.7661326131569743\n",
      "2997 14.248684 0.7410404994880763\n",
      "3330 13.471622 0.716770194155699\n",
      "3663 6.231778 0.6932947815738983\n",
      "3996 11.107171 0.670588227686808\n",
      "4329 11.821829 0.6486253510970671\n",
      "4662 13.661849 0.6273817951398404\n",
      "4995 14.910696 0.6068340008714599\n",
      "5328 10.649235 0.5869591809427341\n",
      "5661 10.137462 0.5677352943279493\n",
      "5994 11.536904 0.5491410218815375\n",
      "6327 17.509365 0.5311557426953045\n",
      "6660 8.896585 0.5137595112299983\n",
      "6993 8.924464 0.496933035195856\n",
      "7326 9.115854 0.4806576541575994\n",
      "7659 7.055336 0.4649153188401532\n",
      "7992 9.06588 0.4496885711121343\n",
      "8325 10.184616 0.43496052462491586\n",
      "8658 12.971654 0.42071484608579357\n",
      "8991 9.376061 0.40693573714448716\n",
      "9324 7.8670063 0.3936079168728886\n",
      "9657 9.913427 0.3807166048186279\n",
      "9990 8.272741 0.3682475046136629\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "tf.reset_default_graph()\n",
    "agent = NeuralAgent(env.reset().shape, env.action_space.n, learning_rate=1e-3, hidden_size=32, gamma=0.99)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train(sess, env, agent, 10000, explore_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04523102  0.18193199 -0.00183326 -0.31767383] 1 1.0\n",
      "[-0.04159238  0.37708    -0.00818674 -0.61093434] 1 1.0\n",
      "[-0.03405078  0.57231542 -0.02040542 -0.90618452] 1 1.0\n",
      "[-0.02260447  0.76770764 -0.03852911 -1.20521061] 1 1.0\n",
      "[-0.00725032  0.96330578 -0.06263333 -1.50971484] 1 1.0\n",
      "[ 0.0120158   1.15912832 -0.09282762 -1.82127484] 1 1.0\n",
      "[ 0.03519837  1.35515071 -0.12925312 -2.14129464] 1 1.0\n",
      "[ 0.06230138  1.55129075 -0.17207901 -2.47094523] 1 1.0\n",
      "[ 0.09332719  1.74739146 -0.22149792 -2.81109387] 1 1.0\n"
     ]
    }
   ],
   "source": [
    "play(sess, env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
