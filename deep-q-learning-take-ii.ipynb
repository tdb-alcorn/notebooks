{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Number of possible actions: 2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Number of possible actions\n",
    "print('Number of possible actions:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 11.0 Training loss: 1.0315 Explore P: 0.9989\n",
      "Episode: 2 Total reward: 18.0 Training loss: 1.0720 Explore P: 0.9971\n",
      "Episode: 3 Total reward: 11.0 Training loss: 1.0438 Explore P: 0.9960\n",
      "Episode: 4 Total reward: 44.0 Training loss: 1.0128 Explore P: 0.9917\n",
      "Episode: 5 Total reward: 40.0 Training loss: 0.9768 Explore P: 0.9878\n",
      "Episode: 6 Total reward: 28.0 Training loss: 1.0224 Explore P: 0.9851\n",
      "Episode: 7 Total reward: 14.0 Training loss: 1.0101 Explore P: 0.9837\n",
      "Episode: 8 Total reward: 26.0 Training loss: 1.0613 Explore P: 0.9812\n",
      "Episode: 9 Total reward: 44.0 Training loss: 1.0730 Explore P: 0.9769\n",
      "Episode: 10 Total reward: 11.0 Training loss: 0.9692 Explore P: 0.9758\n",
      "Episode: 11 Total reward: 58.0 Training loss: 1.1488 Explore P: 0.9703\n",
      "Episode: 12 Total reward: 21.0 Training loss: 1.0503 Explore P: 0.9682\n",
      "Episode: 13 Total reward: 35.0 Training loss: 0.9060 Explore P: 0.9649\n",
      "Episode: 14 Total reward: 11.0 Training loss: 1.1143 Explore P: 0.9638\n",
      "Episode: 15 Total reward: 8.0 Training loss: 1.2794 Explore P: 0.9631\n",
      "Episode: 16 Total reward: 25.0 Training loss: 1.0614 Explore P: 0.9607\n",
      "Episode: 17 Total reward: 12.0 Training loss: 1.3530 Explore P: 0.9596\n",
      "Episode: 18 Total reward: 36.0 Training loss: 1.2920 Explore P: 0.9562\n",
      "Episode: 19 Total reward: 12.0 Training loss: 1.4158 Explore P: 0.9550\n",
      "Episode: 20 Total reward: 39.0 Training loss: 1.7579 Explore P: 0.9513\n",
      "Episode: 21 Total reward: 12.0 Training loss: 1.0752 Explore P: 0.9502\n",
      "Episode: 22 Total reward: 25.0 Training loss: 2.2031 Explore P: 0.9479\n",
      "Episode: 23 Total reward: 21.0 Training loss: 2.0185 Explore P: 0.9459\n",
      "Episode: 24 Total reward: 15.0 Training loss: 1.4591 Explore P: 0.9445\n",
      "Episode: 25 Total reward: 16.0 Training loss: 1.2359 Explore P: 0.9430\n",
      "Episode: 26 Total reward: 13.0 Training loss: 1.9538 Explore P: 0.9418\n",
      "Episode: 27 Total reward: 11.0 Training loss: 1.9799 Explore P: 0.9408\n",
      "Episode: 28 Total reward: 29.0 Training loss: 1.7158 Explore P: 0.9381\n",
      "Episode: 29 Total reward: 22.0 Training loss: 1.4637 Explore P: 0.9360\n",
      "Episode: 30 Total reward: 12.0 Training loss: 1.5939 Explore P: 0.9349\n",
      "Episode: 31 Total reward: 9.0 Training loss: 1.7771 Explore P: 0.9341\n",
      "Episode: 32 Total reward: 20.0 Training loss: 2.2112 Explore P: 0.9322\n",
      "Episode: 33 Total reward: 9.0 Training loss: 1.5467 Explore P: 0.9314\n",
      "Episode: 34 Total reward: 16.0 Training loss: 2.5016 Explore P: 0.9299\n",
      "Episode: 35 Total reward: 12.0 Training loss: 4.8430 Explore P: 0.9288\n",
      "Episode: 36 Total reward: 12.0 Training loss: 5.1880 Explore P: 0.9277\n",
      "Episode: 37 Total reward: 43.0 Training loss: 2.7438 Explore P: 0.9238\n",
      "Episode: 38 Total reward: 34.0 Training loss: 4.6059 Explore P: 0.9207\n",
      "Episode: 39 Total reward: 9.0 Training loss: 7.0274 Explore P: 0.9199\n",
      "Episode: 40 Total reward: 20.0 Training loss: 5.4305 Explore P: 0.9181\n",
      "Episode: 41 Total reward: 10.0 Training loss: 2.5291 Explore P: 0.9171\n",
      "Episode: 42 Total reward: 24.0 Training loss: 23.0215 Explore P: 0.9150\n",
      "Episode: 43 Total reward: 11.0 Training loss: 11.6489 Explore P: 0.9140\n",
      "Episode: 44 Total reward: 8.0 Training loss: 13.5051 Explore P: 0.9133\n",
      "Episode: 45 Total reward: 15.0 Training loss: 2.6309 Explore P: 0.9119\n",
      "Episode: 46 Total reward: 20.0 Training loss: 11.7512 Explore P: 0.9101\n",
      "Episode: 47 Total reward: 11.0 Training loss: 7.9599 Explore P: 0.9091\n",
      "Episode: 48 Total reward: 31.0 Training loss: 12.3799 Explore P: 0.9063\n",
      "Episode: 49 Total reward: 12.0 Training loss: 3.6819 Explore P: 0.9053\n",
      "Episode: 50 Total reward: 34.0 Training loss: 39.5747 Explore P: 0.9022\n",
      "Episode: 51 Total reward: 9.0 Training loss: 14.5041 Explore P: 0.9014\n",
      "Episode: 52 Total reward: 30.0 Training loss: 31.7890 Explore P: 0.8987\n",
      "Episode: 53 Total reward: 13.0 Training loss: 4.4372 Explore P: 0.8976\n",
      "Episode: 54 Total reward: 19.0 Training loss: 11.5277 Explore P: 0.8959\n",
      "Episode: 55 Total reward: 79.0 Training loss: 11.3596 Explore P: 0.8889\n",
      "Episode: 56 Total reward: 16.0 Training loss: 3.5878 Explore P: 0.8875\n",
      "Episode: 57 Total reward: 15.0 Training loss: 18.3908 Explore P: 0.8862\n",
      "Episode: 58 Total reward: 21.0 Training loss: 4.2074 Explore P: 0.8844\n",
      "Episode: 59 Total reward: 12.0 Training loss: 10.2057 Explore P: 0.8833\n",
      "Episode: 60 Total reward: 11.0 Training loss: 4.6828 Explore P: 0.8824\n",
      "Episode: 61 Total reward: 20.0 Training loss: 4.2246 Explore P: 0.8806\n",
      "Episode: 62 Total reward: 14.0 Training loss: 21.4317 Explore P: 0.8794\n",
      "Episode: 63 Total reward: 18.0 Training loss: 15.5692 Explore P: 0.8778\n",
      "Episode: 64 Total reward: 12.0 Training loss: 4.7071 Explore P: 0.8768\n",
      "Episode: 65 Total reward: 18.0 Training loss: 33.2203 Explore P: 0.8752\n",
      "Episode: 66 Total reward: 19.0 Training loss: 68.4174 Explore P: 0.8736\n",
      "Episode: 67 Total reward: 13.0 Training loss: 36.1595 Explore P: 0.8725\n",
      "Episode: 68 Total reward: 14.0 Training loss: 30.5385 Explore P: 0.8713\n",
      "Episode: 69 Total reward: 16.0 Training loss: 25.6296 Explore P: 0.8699\n",
      "Episode: 70 Total reward: 12.0 Training loss: 4.9637 Explore P: 0.8689\n",
      "Episode: 71 Total reward: 15.0 Training loss: 22.1697 Explore P: 0.8676\n",
      "Episode: 72 Total reward: 47.0 Training loss: 5.8164 Explore P: 0.8636\n",
      "Episode: 73 Total reward: 22.0 Training loss: 5.6018 Explore P: 0.8617\n",
      "Episode: 74 Total reward: 19.0 Training loss: 111.5973 Explore P: 0.8601\n",
      "Episode: 75 Total reward: 11.0 Training loss: 75.6796 Explore P: 0.8591\n",
      "Episode: 76 Total reward: 8.0 Training loss: 6.0314 Explore P: 0.8584\n",
      "Episode: 77 Total reward: 22.0 Training loss: 33.4158 Explore P: 0.8566\n",
      "Episode: 78 Total reward: 17.0 Training loss: 21.9784 Explore P: 0.8551\n",
      "Episode: 79 Total reward: 12.0 Training loss: 4.2328 Explore P: 0.8541\n",
      "Episode: 80 Total reward: 11.0 Training loss: 90.1078 Explore P: 0.8532\n",
      "Episode: 81 Total reward: 7.0 Training loss: 21.8390 Explore P: 0.8526\n",
      "Episode: 82 Total reward: 10.0 Training loss: 5.8348 Explore P: 0.8518\n",
      "Episode: 83 Total reward: 12.0 Training loss: 5.2202 Explore P: 0.8508\n",
      "Episode: 84 Total reward: 9.0 Training loss: 35.1793 Explore P: 0.8500\n",
      "Episode: 85 Total reward: 19.0 Training loss: 4.2659 Explore P: 0.8484\n",
      "Episode: 86 Total reward: 8.0 Training loss: 70.7787 Explore P: 0.8477\n",
      "Episode: 87 Total reward: 14.0 Training loss: 110.1482 Explore P: 0.8466\n",
      "Episode: 88 Total reward: 13.0 Training loss: 102.6670 Explore P: 0.8455\n",
      "Episode: 89 Total reward: 23.0 Training loss: 55.7769 Explore P: 0.8436\n",
      "Episode: 90 Total reward: 17.0 Training loss: 34.1457 Explore P: 0.8421\n",
      "Episode: 91 Total reward: 9.0 Training loss: 5.0889 Explore P: 0.8414\n",
      "Episode: 92 Total reward: 19.0 Training loss: 35.3605 Explore P: 0.8398\n",
      "Episode: 93 Total reward: 15.0 Training loss: 6.7358 Explore P: 0.8386\n",
      "Episode: 94 Total reward: 8.0 Training loss: 120.3558 Explore P: 0.8379\n",
      "Episode: 95 Total reward: 17.0 Training loss: 54.4669 Explore P: 0.8365\n",
      "Episode: 96 Total reward: 15.0 Training loss: 184.3096 Explore P: 0.8353\n",
      "Episode: 97 Total reward: 9.0 Training loss: 28.7799 Explore P: 0.8345\n",
      "Episode: 98 Total reward: 9.0 Training loss: 129.4278 Explore P: 0.8338\n",
      "Episode: 99 Total reward: 9.0 Training loss: 111.8919 Explore P: 0.8330\n",
      "Episode: 100 Total reward: 16.0 Training loss: 38.1689 Explore P: 0.8317\n",
      "Episode: 101 Total reward: 33.0 Training loss: 71.9787 Explore P: 0.8290\n",
      "Episode: 102 Total reward: 14.0 Training loss: 4.6514 Explore P: 0.8279\n",
      "Episode: 103 Total reward: 12.0 Training loss: 4.1037 Explore P: 0.8269\n",
      "Episode: 104 Total reward: 8.0 Training loss: 4.9368 Explore P: 0.8262\n",
      "Episode: 105 Total reward: 15.0 Training loss: 5.3071 Explore P: 0.8250\n",
      "Episode: 106 Total reward: 9.0 Training loss: 79.8886 Explore P: 0.8243\n",
      "Episode: 107 Total reward: 13.0 Training loss: 74.4633 Explore P: 0.8232\n",
      "Episode: 108 Total reward: 18.0 Training loss: 67.8646 Explore P: 0.8218\n",
      "Episode: 109 Total reward: 21.0 Training loss: 69.9034 Explore P: 0.8201\n",
      "Episode: 110 Total reward: 10.0 Training loss: 5.7358 Explore P: 0.8192\n",
      "Episode: 111 Total reward: 16.0 Training loss: 6.0669 Explore P: 0.8180\n",
      "Episode: 112 Total reward: 17.0 Training loss: 132.8249 Explore P: 0.8166\n",
      "Episode: 113 Total reward: 18.0 Training loss: 5.5881 Explore P: 0.8151\n",
      "Episode: 114 Total reward: 22.0 Training loss: 5.6918 Explore P: 0.8134\n",
      "Episode: 115 Total reward: 29.0 Training loss: 104.7318 Explore P: 0.8110\n",
      "Episode: 116 Total reward: 18.0 Training loss: 7.3236 Explore P: 0.8096\n",
      "Episode: 117 Total reward: 14.0 Training loss: 4.2513 Explore P: 0.8085\n",
      "Episode: 118 Total reward: 8.0 Training loss: 118.7202 Explore P: 0.8078\n",
      "Episode: 119 Total reward: 14.0 Training loss: 40.5677 Explore P: 0.8067\n",
      "Episode: 120 Total reward: 41.0 Training loss: 146.7062 Explore P: 0.8035\n",
      "Episode: 121 Total reward: 22.0 Training loss: 50.9999 Explore P: 0.8017\n",
      "Episode: 122 Total reward: 20.0 Training loss: 43.9722 Explore P: 0.8001\n",
      "Episode: 123 Total reward: 15.0 Training loss: 49.3884 Explore P: 0.7990\n",
      "Episode: 124 Total reward: 19.0 Training loss: 52.6844 Explore P: 0.7975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 125 Total reward: 13.0 Training loss: 112.2249 Explore P: 0.7964\n",
      "Episode: 126 Total reward: 47.0 Training loss: 115.4710 Explore P: 0.7927\n",
      "Episode: 127 Total reward: 9.0 Training loss: 38.7038 Explore P: 0.7920\n",
      "Episode: 128 Total reward: 25.0 Training loss: 2.8918 Explore P: 0.7901\n",
      "Episode: 129 Total reward: 9.0 Training loss: 91.3488 Explore P: 0.7894\n",
      "Episode: 130 Total reward: 11.0 Training loss: 41.5676 Explore P: 0.7885\n",
      "Episode: 131 Total reward: 22.0 Training loss: 47.0988 Explore P: 0.7868\n",
      "Episode: 132 Total reward: 20.0 Training loss: 51.1520 Explore P: 0.7853\n",
      "Episode: 133 Total reward: 14.0 Training loss: 60.6304 Explore P: 0.7842\n",
      "Episode: 134 Total reward: 13.0 Training loss: 57.0783 Explore P: 0.7832\n",
      "Episode: 135 Total reward: 13.0 Training loss: 46.5582 Explore P: 0.7822\n",
      "Episode: 136 Total reward: 9.0 Training loss: 6.1925 Explore P: 0.7815\n",
      "Episode: 137 Total reward: 9.0 Training loss: 47.4498 Explore P: 0.7808\n",
      "Episode: 138 Total reward: 10.0 Training loss: 60.7637 Explore P: 0.7800\n",
      "Episode: 139 Total reward: 25.0 Training loss: 58.7744 Explore P: 0.7781\n",
      "Episode: 140 Total reward: 33.0 Training loss: 7.1015 Explore P: 0.7756\n",
      "Episode: 141 Total reward: 17.0 Training loss: 44.6209 Explore P: 0.7743\n",
      "Episode: 142 Total reward: 8.0 Training loss: 34.2460 Explore P: 0.7736\n",
      "Episode: 143 Total reward: 15.0 Training loss: 4.4677 Explore P: 0.7725\n",
      "Episode: 144 Total reward: 11.0 Training loss: 34.8154 Explore P: 0.7717\n",
      "Episode: 145 Total reward: 21.0 Training loss: 40.9095 Explore P: 0.7701\n",
      "Episode: 146 Total reward: 8.0 Training loss: 3.0793 Explore P: 0.7695\n",
      "Episode: 147 Total reward: 28.0 Training loss: 35.1168 Explore P: 0.7673\n",
      "Episode: 148 Total reward: 33.0 Training loss: 4.4239 Explore P: 0.7648\n",
      "Episode: 149 Total reward: 15.0 Training loss: 209.8217 Explore P: 0.7637\n",
      "Episode: 150 Total reward: 13.0 Training loss: 104.7122 Explore P: 0.7627\n",
      "Episode: 151 Total reward: 10.0 Training loss: 45.0084 Explore P: 0.7620\n",
      "Episode: 152 Total reward: 9.0 Training loss: 4.4684 Explore P: 0.7613\n",
      "Episode: 153 Total reward: 10.0 Training loss: 3.7758 Explore P: 0.7605\n",
      "Episode: 154 Total reward: 12.0 Training loss: 39.9019 Explore P: 0.7596\n",
      "Episode: 155 Total reward: 12.0 Training loss: 45.0500 Explore P: 0.7587\n",
      "Episode: 156 Total reward: 12.0 Training loss: 33.9861 Explore P: 0.7579\n",
      "Episode: 157 Total reward: 21.0 Training loss: 47.6445 Explore P: 0.7563\n",
      "Episode: 158 Total reward: 25.0 Training loss: 4.5415 Explore P: 0.7544\n",
      "Episode: 159 Total reward: 11.0 Training loss: 118.6734 Explore P: 0.7536\n",
      "Episode: 160 Total reward: 25.0 Training loss: 101.9827 Explore P: 0.7517\n",
      "Episode: 161 Total reward: 9.0 Training loss: 117.5515 Explore P: 0.7511\n",
      "Episode: 162 Total reward: 17.0 Training loss: 7.2063 Explore P: 0.7498\n",
      "Episode: 163 Total reward: 12.0 Training loss: 4.1818 Explore P: 0.7489\n",
      "Episode: 164 Total reward: 8.0 Training loss: 132.1733 Explore P: 0.7483\n",
      "Episode: 165 Total reward: 20.0 Training loss: 3.8698 Explore P: 0.7469\n",
      "Episode: 166 Total reward: 10.0 Training loss: 90.9320 Explore P: 0.7461\n",
      "Episode: 167 Total reward: 10.0 Training loss: 3.9850 Explore P: 0.7454\n",
      "Episode: 168 Total reward: 31.0 Training loss: 237.5399 Explore P: 0.7431\n",
      "Episode: 169 Total reward: 22.0 Training loss: 159.0252 Explore P: 0.7415\n",
      "Episode: 170 Total reward: 10.0 Training loss: 162.7725 Explore P: 0.7408\n",
      "Episode: 171 Total reward: 13.0 Training loss: 79.0732 Explore P: 0.7398\n",
      "Episode: 172 Total reward: 19.0 Training loss: 33.1073 Explore P: 0.7384\n",
      "Episode: 173 Total reward: 14.0 Training loss: 64.2222 Explore P: 0.7374\n",
      "Episode: 174 Total reward: 20.0 Training loss: 47.4868 Explore P: 0.7360\n",
      "Episode: 175 Total reward: 13.0 Training loss: 44.3379 Explore P: 0.7350\n",
      "Episode: 176 Total reward: 13.0 Training loss: 4.9377 Explore P: 0.7341\n",
      "Episode: 177 Total reward: 9.0 Training loss: 3.3241 Explore P: 0.7334\n",
      "Episode: 178 Total reward: 12.0 Training loss: 89.1180 Explore P: 0.7326\n",
      "Episode: 179 Total reward: 17.0 Training loss: 3.7964 Explore P: 0.7313\n",
      "Episode: 180 Total reward: 11.0 Training loss: 37.8730 Explore P: 0.7305\n",
      "Episode: 181 Total reward: 23.0 Training loss: 50.3724 Explore P: 0.7289\n",
      "Episode: 182 Total reward: 10.0 Training loss: 3.0596 Explore P: 0.7282\n",
      "Episode: 183 Total reward: 11.0 Training loss: 34.8552 Explore P: 0.7274\n",
      "Episode: 184 Total reward: 11.0 Training loss: 42.5854 Explore P: 0.7266\n",
      "Episode: 185 Total reward: 9.0 Training loss: 42.3554 Explore P: 0.7259\n",
      "Episode: 186 Total reward: 10.0 Training loss: 3.6740 Explore P: 0.7252\n",
      "Episode: 187 Total reward: 8.0 Training loss: 4.9197 Explore P: 0.7247\n",
      "Episode: 188 Total reward: 14.0 Training loss: 99.5154 Explore P: 0.7237\n",
      "Episode: 189 Total reward: 9.0 Training loss: 92.9735 Explore P: 0.7230\n",
      "Episode: 190 Total reward: 13.0 Training loss: 3.6096 Explore P: 0.7221\n",
      "Episode: 191 Total reward: 14.0 Training loss: 96.9734 Explore P: 0.7211\n",
      "Episode: 192 Total reward: 15.0 Training loss: 32.8672 Explore P: 0.7200\n",
      "Episode: 193 Total reward: 13.0 Training loss: 50.3386 Explore P: 0.7191\n",
      "Episode: 194 Total reward: 13.0 Training loss: 103.0695 Explore P: 0.7182\n",
      "Episode: 195 Total reward: 11.0 Training loss: 78.6989 Explore P: 0.7174\n",
      "Episode: 196 Total reward: 9.0 Training loss: 201.5518 Explore P: 0.7168\n",
      "Episode: 197 Total reward: 18.0 Training loss: 150.4184 Explore P: 0.7155\n",
      "Episode: 198 Total reward: 18.0 Training loss: 2.6911 Explore P: 0.7142\n",
      "Episode: 199 Total reward: 14.0 Training loss: 4.5060 Explore P: 0.7132\n",
      "Episode: 200 Total reward: 14.0 Training loss: 51.4471 Explore P: 0.7123\n",
      "Episode: 201 Total reward: 16.0 Training loss: 54.3394 Explore P: 0.7111\n",
      "Episode: 202 Total reward: 14.0 Training loss: 47.3769 Explore P: 0.7102\n",
      "Episode: 203 Total reward: 11.0 Training loss: 24.9122 Explore P: 0.7094\n",
      "Episode: 204 Total reward: 15.0 Training loss: 5.0488 Explore P: 0.7083\n",
      "Episode: 205 Total reward: 10.0 Training loss: 35.8573 Explore P: 0.7076\n",
      "Episode: 206 Total reward: 32.0 Training loss: 92.1349 Explore P: 0.7054\n",
      "Episode: 207 Total reward: 8.0 Training loss: 92.0873 Explore P: 0.7049\n",
      "Episode: 208 Total reward: 20.0 Training loss: 55.7942 Explore P: 0.7035\n",
      "Episode: 209 Total reward: 16.0 Training loss: 33.5567 Explore P: 0.7024\n",
      "Episode: 210 Total reward: 12.0 Training loss: 38.3167 Explore P: 0.7015\n",
      "Episode: 211 Total reward: 11.0 Training loss: 58.3060 Explore P: 0.7008\n",
      "Episode: 212 Total reward: 32.0 Training loss: 33.3699 Explore P: 0.6986\n",
      "Episode: 213 Total reward: 11.0 Training loss: 2.2944 Explore P: 0.6978\n",
      "Episode: 214 Total reward: 8.0 Training loss: 2.8689 Explore P: 0.6973\n",
      "Episode: 215 Total reward: 16.0 Training loss: 44.2855 Explore P: 0.6962\n",
      "Episode: 216 Total reward: 12.0 Training loss: 30.7403 Explore P: 0.6953\n",
      "Episode: 217 Total reward: 8.0 Training loss: 2.6758 Explore P: 0.6948\n",
      "Episode: 218 Total reward: 17.0 Training loss: 42.3106 Explore P: 0.6936\n",
      "Episode: 219 Total reward: 18.0 Training loss: 60.2644 Explore P: 0.6924\n",
      "Episode: 220 Total reward: 30.0 Training loss: 1.8247 Explore P: 0.6903\n",
      "Episode: 221 Total reward: 32.0 Training loss: 46.3194 Explore P: 0.6882\n",
      "Episode: 222 Total reward: 10.0 Training loss: 119.7540 Explore P: 0.6875\n",
      "Episode: 223 Total reward: 8.0 Training loss: 86.3234 Explore P: 0.6870\n",
      "Episode: 224 Total reward: 12.0 Training loss: 47.2959 Explore P: 0.6861\n",
      "Episode: 225 Total reward: 12.0 Training loss: 22.2265 Explore P: 0.6853\n",
      "Episode: 226 Total reward: 23.0 Training loss: 65.2399 Explore P: 0.6838\n",
      "Episode: 227 Total reward: 21.0 Training loss: 22.2089 Explore P: 0.6824\n",
      "Episode: 228 Total reward: 7.0 Training loss: 97.6694 Explore P: 0.6819\n",
      "Episode: 229 Total reward: 22.0 Training loss: 3.8329 Explore P: 0.6804\n",
      "Episode: 230 Total reward: 21.0 Training loss: 3.6745 Explore P: 0.6790\n",
      "Episode: 231 Total reward: 15.0 Training loss: 25.2476 Explore P: 0.6780\n",
      "Episode: 232 Total reward: 12.0 Training loss: 43.4908 Explore P: 0.6772\n",
      "Episode: 233 Total reward: 8.0 Training loss: 2.6347 Explore P: 0.6767\n",
      "Episode: 234 Total reward: 10.0 Training loss: 4.0006 Explore P: 0.6760\n",
      "Episode: 235 Total reward: 32.0 Training loss: 60.9718 Explore P: 0.6739\n",
      "Episode: 236 Total reward: 10.0 Training loss: 6.1213 Explore P: 0.6732\n",
      "Episode: 237 Total reward: 19.0 Training loss: 2.6784 Explore P: 0.6720\n",
      "Episode: 238 Total reward: 12.0 Training loss: 91.0065 Explore P: 0.6712\n",
      "Episode: 239 Total reward: 8.0 Training loss: 4.8799 Explore P: 0.6706\n",
      "Episode: 240 Total reward: 18.0 Training loss: 23.4963 Explore P: 0.6694\n",
      "Episode: 241 Total reward: 19.0 Training loss: 24.8265 Explore P: 0.6682\n",
      "Episode: 242 Total reward: 22.0 Training loss: 23.4171 Explore P: 0.6668\n",
      "Episode: 243 Total reward: 9.0 Training loss: 3.5376 Explore P: 0.6662\n",
      "Episode: 244 Total reward: 20.0 Training loss: 20.8293 Explore P: 0.6648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 245 Total reward: 12.0 Training loss: 39.2709 Explore P: 0.6641\n",
      "Episode: 246 Total reward: 12.0 Training loss: 48.6901 Explore P: 0.6633\n",
      "Episode: 247 Total reward: 26.0 Training loss: 2.6709 Explore P: 0.6616\n",
      "Episode: 248 Total reward: 8.0 Training loss: 2.7375 Explore P: 0.6611\n",
      "Episode: 249 Total reward: 13.0 Training loss: 25.8094 Explore P: 0.6602\n",
      "Episode: 250 Total reward: 10.0 Training loss: 23.1138 Explore P: 0.6596\n",
      "Episode: 251 Total reward: 17.0 Training loss: 37.3459 Explore P: 0.6585\n",
      "Episode: 252 Total reward: 14.0 Training loss: 2.3844 Explore P: 0.6576\n",
      "Episode: 253 Total reward: 15.0 Training loss: 3.6373 Explore P: 0.6566\n",
      "Episode: 254 Total reward: 25.0 Training loss: 19.4205 Explore P: 0.6550\n",
      "Episode: 255 Total reward: 12.0 Training loss: 2.9355 Explore P: 0.6542\n",
      "Episode: 256 Total reward: 15.0 Training loss: 95.2248 Explore P: 0.6532\n",
      "Episode: 257 Total reward: 18.0 Training loss: 39.9295 Explore P: 0.6521\n",
      "Episode: 258 Total reward: 11.0 Training loss: 2.8798 Explore P: 0.6514\n",
      "Episode: 259 Total reward: 13.0 Training loss: 95.2143 Explore P: 0.6505\n",
      "Episode: 260 Total reward: 12.0 Training loss: 82.5036 Explore P: 0.6498\n",
      "Episode: 261 Total reward: 22.0 Training loss: 2.0275 Explore P: 0.6484\n",
      "Episode: 262 Total reward: 9.0 Training loss: 3.2130 Explore P: 0.6478\n",
      "Episode: 263 Total reward: 17.0 Training loss: 71.6350 Explore P: 0.6467\n",
      "Episode: 264 Total reward: 16.0 Training loss: 2.1812 Explore P: 0.6457\n",
      "Episode: 265 Total reward: 27.0 Training loss: 74.9825 Explore P: 0.6440\n",
      "Episode: 266 Total reward: 9.0 Training loss: 2.8571 Explore P: 0.6434\n",
      "Episode: 267 Total reward: 14.0 Training loss: 3.0892 Explore P: 0.6425\n",
      "Episode: 268 Total reward: 18.0 Training loss: 58.7228 Explore P: 0.6414\n",
      "Episode: 269 Total reward: 11.0 Training loss: 24.1208 Explore P: 0.6407\n",
      "Episode: 270 Total reward: 14.0 Training loss: 87.9394 Explore P: 0.6398\n",
      "Episode: 271 Total reward: 10.0 Training loss: 21.5427 Explore P: 0.6392\n",
      "Episode: 272 Total reward: 16.0 Training loss: 21.7376 Explore P: 0.6382\n",
      "Episode: 273 Total reward: 38.0 Training loss: 2.5982 Explore P: 0.6358\n",
      "Episode: 274 Total reward: 12.0 Training loss: 75.1597 Explore P: 0.6350\n",
      "Episode: 275 Total reward: 11.0 Training loss: 76.4794 Explore P: 0.6343\n",
      "Episode: 276 Total reward: 12.0 Training loss: 41.4623 Explore P: 0.6336\n",
      "Episode: 277 Total reward: 12.0 Training loss: 75.1051 Explore P: 0.6328\n",
      "Episode: 278 Total reward: 10.0 Training loss: 1.7071 Explore P: 0.6322\n",
      "Episode: 279 Total reward: 16.0 Training loss: 2.8456 Explore P: 0.6312\n",
      "Episode: 280 Total reward: 11.0 Training loss: 3.0020 Explore P: 0.6305\n",
      "Episode: 281 Total reward: 11.0 Training loss: 66.6620 Explore P: 0.6299\n",
      "Episode: 282 Total reward: 24.0 Training loss: 63.9347 Explore P: 0.6284\n",
      "Episode: 283 Total reward: 19.0 Training loss: 1.8979 Explore P: 0.6272\n",
      "Episode: 284 Total reward: 8.0 Training loss: 35.4073 Explore P: 0.6267\n",
      "Episode: 285 Total reward: 9.0 Training loss: 54.3541 Explore P: 0.6262\n",
      "Episode: 286 Total reward: 14.0 Training loss: 104.2728 Explore P: 0.6253\n",
      "Episode: 287 Total reward: 10.0 Training loss: 64.8903 Explore P: 0.6247\n",
      "Episode: 288 Total reward: 12.0 Training loss: 34.7229 Explore P: 0.6239\n",
      "Episode: 289 Total reward: 10.0 Training loss: 55.0386 Explore P: 0.6233\n",
      "Episode: 290 Total reward: 12.0 Training loss: 19.5434 Explore P: 0.6226\n",
      "Episode: 291 Total reward: 14.0 Training loss: 104.1387 Explore P: 0.6217\n",
      "Episode: 292 Total reward: 8.0 Training loss: 32.4074 Explore P: 0.6212\n",
      "Episode: 293 Total reward: 17.0 Training loss: 44.8621 Explore P: 0.6202\n",
      "Episode: 294 Total reward: 9.0 Training loss: 3.6126 Explore P: 0.6197\n",
      "Episode: 295 Total reward: 21.0 Training loss: 13.3624 Explore P: 0.6184\n",
      "Episode: 296 Total reward: 37.0 Training loss: 89.9059 Explore P: 0.6161\n",
      "Episode: 297 Total reward: 12.0 Training loss: 1.3132 Explore P: 0.6154\n",
      "Episode: 298 Total reward: 9.0 Training loss: 75.3594 Explore P: 0.6149\n",
      "Episode: 299 Total reward: 13.0 Training loss: 0.9215 Explore P: 0.6141\n",
      "Episode: 300 Total reward: 14.0 Training loss: 35.3388 Explore P: 0.6132\n",
      "Episode: 301 Total reward: 15.0 Training loss: 40.7800 Explore P: 0.6123\n",
      "Episode: 302 Total reward: 33.0 Training loss: 1.4615 Explore P: 0.6103\n",
      "Episode: 303 Total reward: 19.0 Training loss: 22.8735 Explore P: 0.6092\n",
      "Episode: 304 Total reward: 10.0 Training loss: 60.6506 Explore P: 0.6086\n",
      "Episode: 305 Total reward: 10.0 Training loss: 13.3739 Explore P: 0.6080\n",
      "Episode: 306 Total reward: 16.0 Training loss: 1.1410 Explore P: 0.6071\n",
      "Episode: 307 Total reward: 9.0 Training loss: 23.0008 Explore P: 0.6065\n",
      "Episode: 308 Total reward: 12.0 Training loss: 13.8350 Explore P: 0.6058\n",
      "Episode: 309 Total reward: 17.0 Training loss: 1.9193 Explore P: 0.6048\n",
      "Episode: 310 Total reward: 9.0 Training loss: 1.0821 Explore P: 0.6043\n",
      "Episode: 311 Total reward: 11.0 Training loss: 28.9173 Explore P: 0.6036\n",
      "Episode: 312 Total reward: 10.0 Training loss: 2.0556 Explore P: 0.6030\n",
      "Episode: 313 Total reward: 11.0 Training loss: 21.9990 Explore P: 0.6024\n",
      "Episode: 314 Total reward: 16.0 Training loss: 48.8087 Explore P: 0.6014\n",
      "Episode: 315 Total reward: 10.0 Training loss: 28.0934 Explore P: 0.6008\n",
      "Episode: 316 Total reward: 10.0 Training loss: 23.7553 Explore P: 0.6002\n",
      "Episode: 317 Total reward: 10.0 Training loss: 2.1510 Explore P: 0.5996\n",
      "Episode: 318 Total reward: 12.0 Training loss: 12.7418 Explore P: 0.5989\n",
      "Episode: 319 Total reward: 15.0 Training loss: 13.0424 Explore P: 0.5980\n",
      "Episode: 320 Total reward: 24.0 Training loss: 32.5425 Explore P: 0.5966\n",
      "Episode: 321 Total reward: 12.0 Training loss: 41.4053 Explore P: 0.5959\n",
      "Episode: 322 Total reward: 15.0 Training loss: 1.7618 Explore P: 0.5951\n",
      "Episode: 323 Total reward: 18.0 Training loss: 53.0053 Explore P: 0.5940\n",
      "Episode: 324 Total reward: 9.0 Training loss: 57.5681 Explore P: 0.5935\n",
      "Episode: 325 Total reward: 14.0 Training loss: 9.5037 Explore P: 0.5927\n",
      "Episode: 326 Total reward: 12.0 Training loss: 18.0708 Explore P: 0.5920\n",
      "Episode: 327 Total reward: 12.0 Training loss: 1.7522 Explore P: 0.5913\n",
      "Episode: 328 Total reward: 11.0 Training loss: 20.5109 Explore P: 0.5906\n",
      "Episode: 329 Total reward: 13.0 Training loss: 1.2415 Explore P: 0.5899\n",
      "Episode: 330 Total reward: 24.0 Training loss: 2.4385 Explore P: 0.5885\n",
      "Episode: 331 Total reward: 27.0 Training loss: 20.3736 Explore P: 0.5869\n",
      "Episode: 332 Total reward: 26.0 Training loss: 1.4510 Explore P: 0.5854\n",
      "Episode: 333 Total reward: 10.0 Training loss: 1.3421 Explore P: 0.5848\n",
      "Episode: 334 Total reward: 15.0 Training loss: 28.7583 Explore P: 0.5840\n",
      "Episode: 335 Total reward: 15.0 Training loss: 24.0162 Explore P: 0.5831\n",
      "Episode: 336 Total reward: 15.0 Training loss: 26.5748 Explore P: 0.5823\n",
      "Episode: 337 Total reward: 16.0 Training loss: 29.7276 Explore P: 0.5814\n",
      "Episode: 338 Total reward: 13.0 Training loss: 8.7754 Explore P: 0.5806\n",
      "Episode: 339 Total reward: 13.0 Training loss: 15.7607 Explore P: 0.5799\n",
      "Episode: 340 Total reward: 11.0 Training loss: 1.7022 Explore P: 0.5792\n",
      "Episode: 341 Total reward: 13.0 Training loss: 10.9145 Explore P: 0.5785\n",
      "Episode: 342 Total reward: 42.0 Training loss: 66.1187 Explore P: 0.5761\n",
      "Episode: 343 Total reward: 15.0 Training loss: 53.3940 Explore P: 0.5753\n",
      "Episode: 344 Total reward: 13.0 Training loss: 31.0233 Explore P: 0.5745\n",
      "Episode: 345 Total reward: 20.0 Training loss: 33.8577 Explore P: 0.5734\n",
      "Episode: 346 Total reward: 12.0 Training loss: 15.7001 Explore P: 0.5727\n",
      "Episode: 347 Total reward: 8.0 Training loss: 18.0444 Explore P: 0.5723\n",
      "Episode: 348 Total reward: 11.0 Training loss: 50.1654 Explore P: 0.5717\n",
      "Episode: 349 Total reward: 12.0 Training loss: 15.4504 Explore P: 0.5710\n",
      "Episode: 350 Total reward: 31.0 Training loss: 35.9300 Explore P: 0.5693\n",
      "Episode: 351 Total reward: 12.0 Training loss: 16.8256 Explore P: 0.5686\n",
      "Episode: 352 Total reward: 8.0 Training loss: 35.0612 Explore P: 0.5681\n",
      "Episode: 353 Total reward: 36.0 Training loss: 23.9658 Explore P: 0.5661\n",
      "Episode: 354 Total reward: 31.0 Training loss: 34.2260 Explore P: 0.5644\n",
      "Episode: 355 Total reward: 80.0 Training loss: 8.7584 Explore P: 0.5600\n",
      "Episode: 356 Total reward: 29.0 Training loss: 1.6968 Explore P: 0.5584\n",
      "Episode: 357 Total reward: 114.0 Training loss: 10.0591 Explore P: 0.5522\n",
      "Episode: 358 Total reward: 43.0 Training loss: 1.2734 Explore P: 0.5499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 359 Total reward: 73.0 Training loss: 21.2399 Explore P: 0.5459\n",
      "Episode: 360 Total reward: 25.0 Training loss: 12.6680 Explore P: 0.5446\n",
      "Episode: 361 Total reward: 55.0 Training loss: 0.9695 Explore P: 0.5417\n",
      "Episode: 362 Total reward: 35.0 Training loss: 7.7499 Explore P: 0.5398\n",
      "Episode: 363 Total reward: 66.0 Training loss: 8.9419 Explore P: 0.5363\n",
      "Episode: 364 Total reward: 19.0 Training loss: 9.0823 Explore P: 0.5353\n",
      "Episode: 365 Total reward: 21.0 Training loss: 6.3500 Explore P: 0.5342\n",
      "Episode: 366 Total reward: 52.0 Training loss: 9.2004 Explore P: 0.5315\n",
      "Episode: 367 Total reward: 54.0 Training loss: 20.7629 Explore P: 0.5287\n",
      "Episode: 368 Total reward: 38.0 Training loss: 1.7379 Explore P: 0.5267\n",
      "Episode: 369 Total reward: 25.0 Training loss: 5.8685 Explore P: 0.5254\n",
      "Episode: 370 Total reward: 80.0 Training loss: 1.8076 Explore P: 0.5213\n",
      "Episode: 371 Total reward: 41.0 Training loss: 1.6989 Explore P: 0.5192\n",
      "Episode: 372 Total reward: 96.0 Training loss: 7.6388 Explore P: 0.5144\n",
      "Episode: 373 Total reward: 18.0 Training loss: 8.7756 Explore P: 0.5135\n",
      "Episode: 374 Total reward: 19.0 Training loss: 0.7990 Explore P: 0.5125\n",
      "Episode: 375 Total reward: 71.0 Training loss: 7.6961 Explore P: 0.5089\n",
      "Episode: 376 Total reward: 78.0 Training loss: 9.9382 Explore P: 0.5051\n",
      "Episode: 377 Total reward: 35.0 Training loss: 9.1752 Explore P: 0.5033\n",
      "Episode: 378 Total reward: 38.0 Training loss: 10.0515 Explore P: 0.5015\n",
      "Episode: 379 Total reward: 55.0 Training loss: 0.6869 Explore P: 0.4988\n",
      "Episode: 380 Total reward: 43.0 Training loss: 1.5355 Explore P: 0.4967\n",
      "Episode: 381 Total reward: 73.0 Training loss: 21.6496 Explore P: 0.4931\n",
      "Episode: 382 Total reward: 42.0 Training loss: 1.2171 Explore P: 0.4911\n",
      "Episode: 383 Total reward: 27.0 Training loss: 13.1024 Explore P: 0.4898\n",
      "Episode: 384 Total reward: 47.0 Training loss: 2.1154 Explore P: 0.4876\n",
      "Episode: 385 Total reward: 47.0 Training loss: 37.5389 Explore P: 0.4853\n",
      "Episode: 386 Total reward: 52.0 Training loss: 14.0232 Explore P: 0.4829\n",
      "Episode: 387 Total reward: 36.0 Training loss: 8.2284 Explore P: 0.4812\n",
      "Episode: 388 Total reward: 43.0 Training loss: 1.3323 Explore P: 0.4791\n",
      "Episode: 389 Total reward: 34.0 Training loss: 1.3927 Explore P: 0.4775\n",
      "Episode: 390 Total reward: 38.0 Training loss: 21.7485 Explore P: 0.4758\n",
      "Episode: 391 Total reward: 17.0 Training loss: 19.2644 Explore P: 0.4750\n",
      "Episode: 392 Total reward: 26.0 Training loss: 1.7974 Explore P: 0.4738\n",
      "Episode: 393 Total reward: 29.0 Training loss: 27.4529 Explore P: 0.4724\n",
      "Episode: 394 Total reward: 42.0 Training loss: 2.2734 Explore P: 0.4705\n",
      "Episode: 395 Total reward: 32.0 Training loss: 29.8020 Explore P: 0.4690\n",
      "Episode: 396 Total reward: 60.0 Training loss: 26.4615 Explore P: 0.4663\n",
      "Episode: 397 Total reward: 60.0 Training loss: 25.5054 Explore P: 0.4635\n",
      "Episode: 398 Total reward: 35.0 Training loss: 10.8351 Explore P: 0.4620\n",
      "Episode: 399 Total reward: 54.0 Training loss: 1.7013 Explore P: 0.4595\n",
      "Episode: 400 Total reward: 32.0 Training loss: 8.5719 Explore P: 0.4581\n",
      "Episode: 401 Total reward: 42.0 Training loss: 29.4088 Explore P: 0.4562\n",
      "Episode: 402 Total reward: 35.0 Training loss: 1.2011 Explore P: 0.4547\n",
      "Episode: 403 Total reward: 29.0 Training loss: 14.1071 Explore P: 0.4534\n",
      "Episode: 404 Total reward: 33.0 Training loss: 19.2383 Explore P: 0.4519\n",
      "Episode: 405 Total reward: 25.0 Training loss: 1.4556 Explore P: 0.4508\n",
      "Episode: 406 Total reward: 34.0 Training loss: 1.5817 Explore P: 0.4493\n",
      "Episode: 407 Total reward: 84.0 Training loss: 21.4146 Explore P: 0.4456\n",
      "Episode: 408 Total reward: 83.0 Training loss: 0.9574 Explore P: 0.4420\n",
      "Episode: 409 Total reward: 43.0 Training loss: 2.0661 Explore P: 0.4402\n",
      "Episode: 410 Total reward: 34.0 Training loss: 8.0845 Explore P: 0.4387\n",
      "Episode: 412 Total reward: 28.0 Training loss: 38.7057 Explore P: 0.4291\n",
      "Episode: 413 Total reward: 37.0 Training loss: 2.3745 Explore P: 0.4275\n",
      "Episode: 414 Total reward: 64.0 Training loss: 1.1066 Explore P: 0.4248\n",
      "Episode: 415 Total reward: 100.0 Training loss: 17.6683 Explore P: 0.4207\n",
      "Episode: 416 Total reward: 32.0 Training loss: 2.4102 Explore P: 0.4194\n",
      "Episode: 417 Total reward: 25.0 Training loss: 1.9957 Explore P: 0.4184\n",
      "Episode: 418 Total reward: 64.0 Training loss: 15.9272 Explore P: 0.4158\n",
      "Episode: 419 Total reward: 61.0 Training loss: 1.5548 Explore P: 0.4133\n",
      "Episode: 420 Total reward: 43.0 Training loss: 22.4439 Explore P: 0.4116\n",
      "Episode: 421 Total reward: 62.0 Training loss: 0.9162 Explore P: 0.4091\n",
      "Episode: 422 Total reward: 28.0 Training loss: 2.4698 Explore P: 0.4080\n",
      "Episode: 423 Total reward: 116.0 Training loss: 3.0935 Explore P: 0.4034\n",
      "Episode: 424 Total reward: 79.0 Training loss: 1.3172 Explore P: 0.4003\n",
      "Episode: 425 Total reward: 61.0 Training loss: 2.0351 Explore P: 0.3979\n",
      "Episode: 426 Total reward: 52.0 Training loss: 2.8227 Explore P: 0.3959\n",
      "Episode: 427 Total reward: 48.0 Training loss: 2.0476 Explore P: 0.3941\n",
      "Episode: 428 Total reward: 54.0 Training loss: 1.8602 Explore P: 0.3920\n",
      "Episode: 429 Total reward: 59.0 Training loss: 46.5002 Explore P: 0.3897\n",
      "Episode: 430 Total reward: 76.0 Training loss: 2.1748 Explore P: 0.3869\n",
      "Episode: 431 Total reward: 54.0 Training loss: 16.7893 Explore P: 0.3848\n",
      "Episode: 432 Total reward: 84.0 Training loss: 1.8936 Explore P: 0.3817\n",
      "Episode: 433 Total reward: 70.0 Training loss: 2.2748 Explore P: 0.3791\n",
      "Episode: 434 Total reward: 48.0 Training loss: 27.8087 Explore P: 0.3773\n",
      "Episode: 435 Total reward: 45.0 Training loss: 17.2252 Explore P: 0.3757\n",
      "Episode: 436 Total reward: 80.0 Training loss: 43.6595 Explore P: 0.3728\n",
      "Episode: 437 Total reward: 37.0 Training loss: 0.9180 Explore P: 0.3714\n",
      "Episode: 438 Total reward: 69.0 Training loss: 44.7221 Explore P: 0.3690\n",
      "Episode: 439 Total reward: 131.0 Training loss: 46.5417 Explore P: 0.3643\n",
      "Episode: 440 Total reward: 69.0 Training loss: 2.8154 Explore P: 0.3618\n",
      "Episode: 441 Total reward: 120.0 Training loss: 44.6283 Explore P: 0.3577\n",
      "Episode: 442 Total reward: 173.0 Training loss: 59.3633 Explore P: 0.3517\n",
      "Episode: 443 Total reward: 156.0 Training loss: 54.7574 Explore P: 0.3464\n",
      "Episode: 444 Total reward: 40.0 Training loss: 1.6566 Explore P: 0.3451\n",
      "Episode: 445 Total reward: 140.0 Training loss: 20.9318 Explore P: 0.3404\n",
      "Episode: 446 Total reward: 44.0 Training loss: 2.7994 Explore P: 0.3389\n",
      "Episode: 447 Total reward: 49.0 Training loss: 39.2043 Explore P: 0.3373\n",
      "Episode: 448 Total reward: 88.0 Training loss: 25.3999 Explore P: 0.3345\n",
      "Episode: 449 Total reward: 46.0 Training loss: 48.5460 Explore P: 0.3330\n",
      "Episode: 450 Total reward: 80.0 Training loss: 30.9157 Explore P: 0.3304\n",
      "Episode: 451 Total reward: 97.0 Training loss: 3.6524 Explore P: 0.3273\n",
      "Episode: 452 Total reward: 78.0 Training loss: 28.3871 Explore P: 0.3249\n",
      "Episode: 453 Total reward: 68.0 Training loss: 3.2926 Explore P: 0.3227\n",
      "Episode: 454 Total reward: 86.0 Training loss: 2.6025 Explore P: 0.3200\n",
      "Episode: 455 Total reward: 79.0 Training loss: 51.1012 Explore P: 0.3176\n",
      "Episode: 456 Total reward: 44.0 Training loss: 13.5835 Explore P: 0.3163\n",
      "Episode: 457 Total reward: 61.0 Training loss: 90.4003 Explore P: 0.3144\n",
      "Episode: 458 Total reward: 110.0 Training loss: 2.4320 Explore P: 0.3111\n",
      "Episode: 459 Total reward: 87.0 Training loss: 17.6090 Explore P: 0.3085\n",
      "Episode: 460 Total reward: 77.0 Training loss: 23.6641 Explore P: 0.3062\n",
      "Episode: 462 Total reward: 50.0 Training loss: 21.7578 Explore P: 0.2988\n",
      "Episode: 464 Total reward: 21.0 Training loss: 55.2199 Explore P: 0.2925\n",
      "Episode: 465 Total reward: 186.0 Training loss: 7.1791 Explore P: 0.2873\n",
      "Episode: 466 Total reward: 113.0 Training loss: 3.1749 Explore P: 0.2842\n",
      "Episode: 467 Total reward: 127.0 Training loss: 2.1568 Explore P: 0.2808\n",
      "Episode: 468 Total reward: 130.0 Training loss: 3.2152 Explore P: 0.2773\n",
      "Episode: 469 Total reward: 170.0 Training loss: 105.6835 Explore P: 0.2728\n",
      "Episode: 470 Total reward: 131.0 Training loss: 1.3072 Explore P: 0.2693\n",
      "Episode: 471 Total reward: 96.0 Training loss: 1.0483 Explore P: 0.2669\n",
      "Episode: 472 Total reward: 128.0 Training loss: 2.1689 Explore P: 0.2636\n",
      "Episode: 473 Total reward: 56.0 Training loss: 60.5027 Explore P: 0.2622\n",
      "Episode: 474 Total reward: 117.0 Training loss: 25.7877 Explore P: 0.2592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 475 Total reward: 174.0 Training loss: 1.5152 Explore P: 0.2549\n",
      "Episode: 476 Total reward: 101.0 Training loss: 52.5082 Explore P: 0.2525\n",
      "Episode: 477 Total reward: 64.0 Training loss: 32.0341 Explore P: 0.2509\n",
      "Episode: 478 Total reward: 92.0 Training loss: 1.9479 Explore P: 0.2487\n",
      "Episode: 479 Total reward: 88.0 Training loss: 1.0063 Explore P: 0.2466\n",
      "Episode: 480 Total reward: 129.0 Training loss: 39.5081 Explore P: 0.2436\n",
      "Episode: 481 Total reward: 147.0 Training loss: 0.9590 Explore P: 0.2402\n",
      "Episode: 483 Total reward: 22.0 Training loss: 2.5352 Explore P: 0.2351\n",
      "Episode: 485 Total reward: 43.0 Training loss: 1.7627 Explore P: 0.2297\n",
      "Episode: 486 Total reward: 130.0 Training loss: 1.0097 Explore P: 0.2269\n",
      "Episode: 487 Total reward: 159.0 Training loss: 1.9966 Explore P: 0.2235\n",
      "Episode: 488 Total reward: 73.0 Training loss: 0.8327 Explore P: 0.2219\n",
      "Episode: 489 Total reward: 136.0 Training loss: 1.1225 Explore P: 0.2191\n",
      "Episode: 491 Total reward: 17.0 Training loss: 0.9376 Explore P: 0.2146\n",
      "Episode: 492 Total reward: 161.0 Training loss: 1.3545 Explore P: 0.2113\n",
      "Episode: 495 Total reward: 34.0 Training loss: 1.0288 Explore P: 0.2028\n",
      "Episode: 498 Total reward: 90.0 Training loss: 0.5546 Explore P: 0.1935\n",
      "Episode: 500 Total reward: 74.0 Training loss: 0.5470 Explore P: 0.1886\n",
      "Episode: 502 Total reward: 50.0 Training loss: 0.8153 Explore P: 0.1842\n",
      "Episode: 504 Total reward: 185.0 Training loss: 1.3733 Explore P: 0.1776\n",
      "Episode: 506 Total reward: 107.0 Training loss: 1.5962 Explore P: 0.1725\n",
      "Episode: 509 Total reward: 80.0 Training loss: 1.2201 Explore P: 0.1649\n",
      "Episode: 511 Total reward: 94.0 Training loss: 1.0737 Explore P: 0.1604\n",
      "Episode: 514 Total reward: 99.0 Training loss: 358.5721 Explore P: 0.1531\n",
      "Episode: 517 Total reward: 99.0 Training loss: 1.3156 Explore P: 0.1461\n",
      "Episode: 520 Total reward: 84.0 Training loss: 0.6509 Explore P: 0.1397\n",
      "Episode: 522 Total reward: 105.0 Training loss: 0.7971 Explore P: 0.1358\n",
      "Episode: 525 Total reward: 99.0 Training loss: 0.6382 Explore P: 0.1297\n",
      "Episode: 528 Total reward: 99.0 Training loss: 0.9465 Explore P: 0.1239\n",
      "Episode: 531 Total reward: 62.0 Training loss: 0.5606 Explore P: 0.1187\n",
      "Episode: 534 Total reward: 22.0 Training loss: 0.5196 Explore P: 0.1142\n",
      "Episode: 536 Total reward: 156.0 Training loss: 0.2790 Explore P: 0.1106\n",
      "Episode: 538 Total reward: 182.0 Training loss: 0.3847 Explore P: 0.1068\n",
      "Episode: 540 Total reward: 60.0 Training loss: 0.7137 Explore P: 0.1043\n",
      "Episode: 542 Total reward: 40.0 Training loss: 0.4389 Explore P: 0.1021\n",
      "Episode: 544 Total reward: 30.0 Training loss: 0.8366 Explore P: 0.1000\n",
      "Episode: 546 Total reward: 30.0 Training loss: 0.7477 Explore P: 0.0979\n",
      "Episode: 548 Total reward: 141.0 Training loss: 0.5940 Explore P: 0.0950\n",
      "Episode: 551 Total reward: 99.0 Training loss: 0.7641 Explore P: 0.0909\n",
      "Episode: 554 Total reward: 99.0 Training loss: 0.5430 Explore P: 0.0869\n",
      "Episode: 556 Total reward: 146.0 Training loss: 0.2717 Explore P: 0.0843\n",
      "Episode: 558 Total reward: 165.0 Training loss: 0.4904 Explore P: 0.0816\n",
      "Episode: 560 Total reward: 102.0 Training loss: 0.2515 Explore P: 0.0795\n",
      "Episode: 562 Total reward: 136.0 Training loss: 0.5925 Explore P: 0.0772\n",
      "Episode: 564 Total reward: 75.0 Training loss: 0.2670 Explore P: 0.0754\n",
      "Episode: 566 Total reward: 80.0 Training loss: 0.2731 Explore P: 0.0736\n",
      "Episode: 568 Total reward: 67.0 Training loss: 0.7401 Explore P: 0.0719\n",
      "Episode: 569 Total reward: 175.0 Training loss: 0.8903 Explore P: 0.0708\n",
      "Episode: 570 Total reward: 120.0 Training loss: 0.3485 Explore P: 0.0701\n",
      "Episode: 571 Total reward: 124.0 Training loss: 0.5672 Explore P: 0.0694\n",
      "Episode: 572 Total reward: 104.0 Training loss: 0.3323 Explore P: 0.0688\n",
      "Episode: 573 Total reward: 114.0 Training loss: 0.5280 Explore P: 0.0681\n",
      "Episode: 574 Total reward: 125.0 Training loss: 0.1910 Explore P: 0.0674\n",
      "Episode: 575 Total reward: 112.0 Training loss: 0.4259 Explore P: 0.0667\n",
      "Episode: 576 Total reward: 114.0 Training loss: 1.2905 Explore P: 0.0661\n",
      "Episode: 577 Total reward: 35.0 Training loss: 0.4347 Explore P: 0.0659\n",
      "Episode: 578 Total reward: 99.0 Training loss: 0.4377 Explore P: 0.0653\n",
      "Episode: 579 Total reward: 42.0 Training loss: 0.6307 Explore P: 0.0651\n",
      "Episode: 580 Total reward: 88.0 Training loss: 0.4340 Explore P: 0.0646\n",
      "Episode: 581 Total reward: 25.0 Training loss: 0.3922 Explore P: 0.0645\n",
      "Episode: 582 Total reward: 30.0 Training loss: 0.3249 Explore P: 0.0643\n",
      "Episode: 583 Total reward: 26.0 Training loss: 0.4815 Explore P: 0.0642\n",
      "Episode: 584 Total reward: 28.0 Training loss: 1.5410 Explore P: 0.0640\n",
      "Episode: 585 Total reward: 30.0 Training loss: 0.7258 Explore P: 0.0639\n",
      "Episode: 586 Total reward: 21.0 Training loss: 0.3775 Explore P: 0.0638\n",
      "Episode: 587 Total reward: 17.0 Training loss: 0.7846 Explore P: 0.0637\n",
      "Episode: 588 Total reward: 18.0 Training loss: 0.9036 Explore P: 0.0636\n",
      "Episode: 589 Total reward: 12.0 Training loss: 1.2750 Explore P: 0.0635\n",
      "Episode: 590 Total reward: 11.0 Training loss: 1.4685 Explore P: 0.0634\n",
      "Episode: 591 Total reward: 17.0 Training loss: 1.2899 Explore P: 0.0634\n",
      "Episode: 592 Total reward: 20.0 Training loss: 1.2053 Explore P: 0.0633\n",
      "Episode: 593 Total reward: 103.0 Training loss: 0.3050 Explore P: 0.0627\n",
      "Episode: 594 Total reward: 102.0 Training loss: 0.8968 Explore P: 0.0622\n",
      "Episode: 595 Total reward: 126.0 Training loss: 0.6941 Explore P: 0.0615\n",
      "Episode: 596 Total reward: 122.0 Training loss: 0.3946 Explore P: 0.0609\n",
      "Episode: 597 Total reward: 101.0 Training loss: 0.4048 Explore P: 0.0604\n",
      "Episode: 598 Total reward: 145.0 Training loss: 0.5442 Explore P: 0.0597\n",
      "Episode: 599 Total reward: 145.0 Training loss: 0.5219 Explore P: 0.0589\n",
      "Episode: 601 Total reward: 4.0 Training loss: 0.9078 Explore P: 0.0580\n",
      "Episode: 602 Total reward: 197.0 Training loss: 0.3713 Explore P: 0.0570\n",
      "Episode: 605 Total reward: 99.0 Training loss: 0.3767 Explore P: 0.0547\n",
      "Episode: 608 Total reward: 99.0 Training loss: 0.7074 Explore P: 0.0526\n",
      "Episode: 611 Total reward: 49.0 Training loss: 0.5642 Explore P: 0.0507\n",
      "Episode: 613 Total reward: 116.0 Training loss: 0.2408 Explore P: 0.0494\n",
      "Episode: 615 Total reward: 157.0 Training loss: 0.2213 Explore P: 0.0480\n",
      "Episode: 617 Total reward: 171.0 Training loss: 0.4520 Explore P: 0.0466\n",
      "Episode: 620 Total reward: 99.0 Training loss: 0.2442 Explore P: 0.0449\n",
      "Episode: 623 Total reward: 99.0 Training loss: 0.0960 Explore P: 0.0432\n",
      "Episode: 626 Total reward: 99.0 Training loss: 0.4948 Explore P: 0.0416\n",
      "Episode: 629 Total reward: 99.0 Training loss: 0.1342 Explore P: 0.0400\n",
      "Episode: 632 Total reward: 99.0 Training loss: 0.1348 Explore P: 0.0386\n",
      "Episode: 635 Total reward: 99.0 Training loss: 0.2109 Explore P: 0.0372\n",
      "Episode: 638 Total reward: 99.0 Training loss: 0.3472 Explore P: 0.0358\n",
      "Episode: 641 Total reward: 99.0 Training loss: 0.8235 Explore P: 0.0346\n",
      "Episode: 644 Total reward: 99.0 Training loss: 0.4154 Explore P: 0.0334\n",
      "Episode: 647 Total reward: 99.0 Training loss: 0.1695 Explore P: 0.0323\n",
      "Episode: 650 Total reward: 99.0 Training loss: 0.5248 Explore P: 0.0312\n",
      "Episode: 653 Total reward: 13.0 Training loss: 0.7437 Explore P: 0.0303\n",
      "Episode: 656 Total reward: 99.0 Training loss: 0.6141 Explore P: 0.0293\n",
      "Episode: 657 Total reward: 23.0 Training loss: 0.3659 Explore P: 0.0293\n",
      "Episode: 660 Total reward: 9.0 Training loss: 0.3312 Explore P: 0.0285\n",
      "Episode: 663 Total reward: 99.0 Training loss: 0.1630 Explore P: 0.0276\n",
      "Episode: 666 Total reward: 99.0 Training loss: 0.0457 Explore P: 0.0267\n",
      "Episode: 669 Total reward: 99.0 Training loss: 0.1469 Explore P: 0.0259\n",
      "Episode: 672 Total reward: 78.0 Training loss: 0.3803 Explore P: 0.0252\n",
      "Episode: 674 Total reward: 194.0 Training loss: 0.0818 Explore P: 0.0246\n",
      "Episode: 676 Total reward: 154.0 Training loss: 0.1627 Explore P: 0.0241\n",
      "Episode: 678 Total reward: 182.0 Training loss: 0.0875 Explore P: 0.0236\n",
      "Episode: 680 Total reward: 128.0 Training loss: 0.1940 Explore P: 0.0231\n",
      "Episode: 682 Total reward: 184.0 Training loss: 0.1393 Explore P: 0.0226\n",
      "Episode: 684 Total reward: 122.0 Training loss: 0.2955 Explore P: 0.0222\n",
      "Episode: 686 Total reward: 49.0 Training loss: 0.2178 Explore P: 0.0219\n",
      "Episode: 688 Total reward: 124.0 Training loss: 0.1576 Explore P: 0.0216\n",
      "Episode: 690 Total reward: 83.0 Training loss: 0.1567 Explore P: 0.0212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 692 Total reward: 89.0 Training loss: 0.2360 Explore P: 0.0209\n",
      "Episode: 694 Total reward: 102.0 Training loss: 0.1549 Explore P: 0.0206\n",
      "Episode: 696 Total reward: 22.0 Training loss: 0.1895 Explore P: 0.0204\n",
      "Episode: 698 Total reward: 15.0 Training loss: 0.1540 Explore P: 0.0201\n",
      "Episode: 700 Total reward: 82.0 Training loss: 0.0952 Explore P: 0.0199\n",
      "Episode: 701 Total reward: 172.0 Training loss: 0.1240 Explore P: 0.0197\n",
      "Episode: 703 Total reward: 80.0 Training loss: 0.1355 Explore P: 0.0194\n",
      "Episode: 705 Total reward: 11.0 Training loss: 0.0967 Explore P: 0.0192\n",
      "Episode: 707 Total reward: 177.0 Training loss: 0.1278 Explore P: 0.0189\n",
      "Episode: 709 Total reward: 35.0 Training loss: 0.0724 Explore P: 0.0187\n",
      "Episode: 710 Total reward: 190.0 Training loss: 0.3937 Explore P: 0.0185\n",
      "Episode: 712 Total reward: 131.0 Training loss: 0.1341 Explore P: 0.0182\n",
      "Episode: 715 Total reward: 37.0 Training loss: 0.0995 Explore P: 0.0179\n",
      "Episode: 716 Total reward: 133.0 Training loss: 0.4001 Explore P: 0.0178\n",
      "Episode: 717 Total reward: 197.0 Training loss: 165.6475 Explore P: 0.0176\n",
      "Episode: 720 Total reward: 99.0 Training loss: 0.2141 Explore P: 0.0173\n",
      "Episode: 723 Total reward: 99.0 Training loss: 0.2083 Explore P: 0.0169\n",
      "Episode: 725 Total reward: 106.0 Training loss: 0.1575 Explore P: 0.0167\n",
      "Episode: 727 Total reward: 99.0 Training loss: 144.5121 Explore P: 0.0165\n",
      "Episode: 728 Total reward: 186.0 Training loss: 0.1658 Explore P: 0.0164\n",
      "Episode: 729 Total reward: 172.0 Training loss: 0.1663 Explore P: 0.0163\n",
      "Episode: 730 Total reward: 183.0 Training loss: 0.2884 Explore P: 0.0162\n",
      "Episode: 732 Total reward: 64.0 Training loss: 0.1062 Explore P: 0.0160\n",
      "Episode: 735 Total reward: 32.0 Training loss: 0.1499 Explore P: 0.0157\n",
      "Episode: 738 Total reward: 77.0 Training loss: 0.1155 Explore P: 0.0155\n",
      "Episode: 741 Total reward: 99.0 Training loss: 0.2125 Explore P: 0.0152\n",
      "Episode: 744 Total reward: 99.0 Training loss: 0.2045 Explore P: 0.0150\n",
      "Episode: 747 Total reward: 99.0 Training loss: 0.1029 Explore P: 0.0147\n",
      "Episode: 750 Total reward: 99.0 Training loss: 0.2198 Explore P: 0.0145\n",
      "Episode: 753 Total reward: 99.0 Training loss: 0.4828 Explore P: 0.0143\n",
      "Episode: 756 Total reward: 99.0 Training loss: 0.0535 Explore P: 0.0141\n",
      "Episode: 759 Total reward: 27.0 Training loss: 0.1218 Explore P: 0.0139\n",
      "Episode: 762 Total reward: 63.0 Training loss: 0.1149 Explore P: 0.0137\n",
      "Episode: 765 Total reward: 34.0 Training loss: 0.2021 Explore P: 0.0136\n",
      "Episode: 768 Total reward: 22.0 Training loss: 0.1247 Explore P: 0.0134\n",
      "Episode: 771 Total reward: 99.0 Training loss: 0.1175 Explore P: 0.0132\n",
      "Episode: 774 Total reward: 99.0 Training loss: 0.0692 Explore P: 0.0131\n",
      "Episode: 777 Total reward: 99.0 Training loss: 0.1244 Explore P: 0.0129\n",
      "Episode: 780 Total reward: 33.0 Training loss: 0.2158 Explore P: 0.0128\n",
      "Episode: 783 Total reward: 99.0 Training loss: 0.1042 Explore P: 0.0127\n",
      "Episode: 786 Total reward: 99.0 Training loss: 0.1067 Explore P: 0.0125\n",
      "Episode: 789 Total reward: 99.0 Training loss: 0.0972 Explore P: 0.0124\n",
      "Episode: 792 Total reward: 99.0 Training loss: 0.1231 Explore P: 0.0123\n",
      "Episode: 795 Total reward: 99.0 Training loss: 0.1418 Explore P: 0.0122\n",
      "Episode: 798 Total reward: 99.0 Training loss: 0.0804 Explore P: 0.0121\n",
      "Episode: 801 Total reward: 99.0 Training loss: 221.2204 Explore P: 0.0120\n",
      "Episode: 804 Total reward: 99.0 Training loss: 0.1246 Explore P: 0.0119\n",
      "Episode: 807 Total reward: 99.0 Training loss: 0.0777 Explore P: 0.0118\n",
      "Episode: 810 Total reward: 99.0 Training loss: 0.0689 Explore P: 0.0117\n",
      "Episode: 813 Total reward: 99.0 Training loss: 0.0759 Explore P: 0.0116\n",
      "Episode: 816 Total reward: 99.0 Training loss: 0.1195 Explore P: 0.0115\n",
      "Episode: 819 Total reward: 99.0 Training loss: 0.2172 Explore P: 0.0115\n",
      "Episode: 822 Total reward: 99.0 Training loss: 0.1113 Explore P: 0.0114\n",
      "Episode: 825 Total reward: 99.0 Training loss: 0.1174 Explore P: 0.0113\n",
      "Episode: 828 Total reward: 99.0 Training loss: 0.1043 Explore P: 0.0113\n",
      "Episode: 831 Total reward: 99.0 Training loss: 0.0541 Explore P: 0.0112\n",
      "Episode: 834 Total reward: 99.0 Training loss: 0.0956 Explore P: 0.0111\n",
      "Episode: 837 Total reward: 99.0 Training loss: 0.1527 Explore P: 0.0111\n",
      "Episode: 840 Total reward: 99.0 Training loss: 0.1188 Explore P: 0.0110\n",
      "Episode: 841 Total reward: 88.0 Training loss: 0.1812 Explore P: 0.0110\n",
      "Episode: 842 Total reward: 115.0 Training loss: 0.1975 Explore P: 0.0110\n",
      "Episode: 843 Total reward: 120.0 Training loss: 0.2634 Explore P: 0.0110\n",
      "Episode: 844 Total reward: 55.0 Training loss: 0.3159 Explore P: 0.0110\n",
      "Episode: 845 Total reward: 76.0 Training loss: 379.3651 Explore P: 0.0110\n",
      "Episode: 846 Total reward: 77.0 Training loss: 0.2345 Explore P: 0.0110\n",
      "Episode: 847 Total reward: 35.0 Training loss: 0.2618 Explore P: 0.0110\n",
      "Episode: 848 Total reward: 65.0 Training loss: 399.8826 Explore P: 0.0110\n",
      "Episode: 849 Total reward: 87.0 Training loss: 323.0268 Explore P: 0.0110\n",
      "Episode: 852 Total reward: 99.0 Training loss: 0.2054 Explore P: 0.0109\n",
      "Episode: 855 Total reward: 99.0 Training loss: 0.0871 Explore P: 0.0109\n",
      "Episode: 858 Total reward: 99.0 Training loss: 0.0967 Explore P: 0.0108\n",
      "Episode: 861 Total reward: 99.0 Training loss: 0.3142 Explore P: 0.0108\n",
      "Episode: 864 Total reward: 99.0 Training loss: 0.1042 Explore P: 0.0108\n",
      "Episode: 867 Total reward: 99.0 Training loss: 0.2217 Explore P: 0.0107\n",
      "Episode: 870 Total reward: 99.0 Training loss: 0.2722 Explore P: 0.0107\n",
      "Episode: 873 Total reward: 99.0 Training loss: 0.1981 Explore P: 0.0106\n",
      "Episode: 876 Total reward: 99.0 Training loss: 0.1054 Explore P: 0.0106\n",
      "Episode: 879 Total reward: 99.0 Training loss: 0.1058 Explore P: 0.0106\n",
      "Episode: 882 Total reward: 99.0 Training loss: 0.2738 Explore P: 0.0106\n",
      "Episode: 885 Total reward: 99.0 Training loss: 0.0997 Explore P: 0.0105\n",
      "Episode: 888 Total reward: 99.0 Training loss: 0.0805 Explore P: 0.0105\n",
      "Episode: 891 Total reward: 99.0 Training loss: 0.7087 Explore P: 0.0105\n",
      "Episode: 894 Total reward: 99.0 Training loss: 0.3391 Explore P: 0.0105\n",
      "Episode: 897 Total reward: 99.0 Training loss: 0.1239 Explore P: 0.0104\n",
      "Episode: 900 Total reward: 99.0 Training loss: 0.0620 Explore P: 0.0104\n",
      "Episode: 903 Total reward: 99.0 Training loss: 0.1928 Explore P: 0.0104\n",
      "Episode: 906 Total reward: 99.0 Training loss: 2.9614 Explore P: 0.0104\n",
      "Episode: 907 Total reward: 15.0 Training loss: 0.2517 Explore P: 0.0104\n",
      "Episode: 908 Total reward: 17.0 Training loss: 0.1849 Explore P: 0.0104\n",
      "Episode: 911 Total reward: 99.0 Training loss: 0.3426 Explore P: 0.0104\n",
      "Episode: 912 Total reward: 14.0 Training loss: 0.2049 Explore P: 0.0104\n",
      "Episode: 913 Total reward: 11.0 Training loss: 0.1965 Explore P: 0.0104\n",
      "Episode: 914 Total reward: 13.0 Training loss: 0.1514 Explore P: 0.0104\n",
      "Episode: 915 Total reward: 8.0 Training loss: 0.1478 Explore P: 0.0104\n",
      "Episode: 916 Total reward: 11.0 Training loss: 0.4056 Explore P: 0.0104\n",
      "Episode: 917 Total reward: 13.0 Training loss: 0.4337 Explore P: 0.0104\n",
      "Episode: 918 Total reward: 10.0 Training loss: 0.2470 Explore P: 0.0104\n",
      "Episode: 919 Total reward: 12.0 Training loss: 0.1836 Explore P: 0.0104\n",
      "Episode: 920 Total reward: 10.0 Training loss: 0.3305 Explore P: 0.0104\n",
      "Episode: 921 Total reward: 11.0 Training loss: 330.2559 Explore P: 0.0104\n",
      "Episode: 922 Total reward: 10.0 Training loss: 0.4059 Explore P: 0.0103\n",
      "Episode: 923 Total reward: 10.0 Training loss: 0.3115 Explore P: 0.0103\n",
      "Episode: 924 Total reward: 9.0 Training loss: 0.1399 Explore P: 0.0103\n",
      "Episode: 925 Total reward: 15.0 Training loss: 0.2085 Explore P: 0.0103\n",
      "Episode: 926 Total reward: 9.0 Training loss: 0.0867 Explore P: 0.0103\n",
      "Episode: 927 Total reward: 10.0 Training loss: 0.1939 Explore P: 0.0103\n",
      "Episode: 928 Total reward: 11.0 Training loss: 0.6029 Explore P: 0.0103\n",
      "Episode: 929 Total reward: 10.0 Training loss: 0.2983 Explore P: 0.0103\n",
      "Episode: 930 Total reward: 14.0 Training loss: 0.3769 Explore P: 0.0103\n",
      "Episode: 931 Total reward: 11.0 Training loss: 316.9947 Explore P: 0.0103\n",
      "Episode: 932 Total reward: 12.0 Training loss: 0.2380 Explore P: 0.0103\n",
      "Episode: 933 Total reward: 14.0 Training loss: 276.4855 Explore P: 0.0103\n",
      "Episode: 934 Total reward: 13.0 Training loss: 0.1973 Explore P: 0.0103\n",
      "Episode: 935 Total reward: 10.0 Training loss: 0.3860 Explore P: 0.0103\n",
      "Episode: 936 Total reward: 11.0 Training loss: 0.9018 Explore P: 0.0103\n",
      "Episode: 937 Total reward: 11.0 Training loss: 0.5257 Explore P: 0.0103\n",
      "Episode: 938 Total reward: 11.0 Training loss: 0.7904 Explore P: 0.0103\n",
      "Episode: 939 Total reward: 14.0 Training loss: 0.4774 Explore P: 0.0103\n",
      "Episode: 940 Total reward: 9.0 Training loss: 0.7912 Explore P: 0.0103\n",
      "Episode: 941 Total reward: 11.0 Training loss: 810.1575 Explore P: 0.0103\n",
      "Episode: 942 Total reward: 12.0 Training loss: 723.3099 Explore P: 0.0103\n",
      "Episode: 943 Total reward: 12.0 Training loss: 0.9913 Explore P: 0.0103\n",
      "Episode: 944 Total reward: 10.0 Training loss: 1.1705 Explore P: 0.0103\n",
      "Episode: 945 Total reward: 16.0 Training loss: 0.7554 Explore P: 0.0103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 946 Total reward: 15.0 Training loss: 795.0436 Explore P: 0.0103\n",
      "Episode: 949 Total reward: 99.0 Training loss: 1.6740 Explore P: 0.0103\n",
      "Episode: 950 Total reward: 50.0 Training loss: 2.3543 Explore P: 0.0103\n",
      "Episode: 951 Total reward: 43.0 Training loss: 3.4401 Explore P: 0.0103\n",
      "Episode: 952 Total reward: 20.0 Training loss: 1.9404 Explore P: 0.0103\n",
      "Episode: 953 Total reward: 13.0 Training loss: 2.2597 Explore P: 0.0103\n",
      "Episode: 954 Total reward: 16.0 Training loss: 1.9588 Explore P: 0.0103\n",
      "Episode: 955 Total reward: 14.0 Training loss: 586.8810 Explore P: 0.0103\n",
      "Episode: 956 Total reward: 14.0 Training loss: 2.4387 Explore P: 0.0103\n",
      "Episode: 957 Total reward: 10.0 Training loss: 4.0468 Explore P: 0.0103\n",
      "Episode: 958 Total reward: 9.0 Training loss: 2.4275 Explore P: 0.0103\n",
      "Episode: 959 Total reward: 12.0 Training loss: 3.2327 Explore P: 0.0103\n",
      "Episode: 960 Total reward: 12.0 Training loss: 2.6738 Explore P: 0.0103\n",
      "Episode: 961 Total reward: 9.0 Training loss: 2.1138 Explore P: 0.0103\n",
      "Episode: 962 Total reward: 8.0 Training loss: 2.8210 Explore P: 0.0103\n",
      "Episode: 963 Total reward: 10.0 Training loss: 4.1212 Explore P: 0.0103\n",
      "Episode: 964 Total reward: 10.0 Training loss: 3.7801 Explore P: 0.0103\n",
      "Episode: 965 Total reward: 7.0 Training loss: 3.8460 Explore P: 0.0103\n",
      "Episode: 966 Total reward: 9.0 Training loss: 784.9379 Explore P: 0.0103\n",
      "Episode: 967 Total reward: 11.0 Training loss: 4.3538 Explore P: 0.0103\n",
      "Episode: 968 Total reward: 8.0 Training loss: 3.3083 Explore P: 0.0103\n",
      "Episode: 969 Total reward: 8.0 Training loss: 2.8459 Explore P: 0.0103\n",
      "Episode: 970 Total reward: 12.0 Training loss: 2.7220 Explore P: 0.0103\n",
      "Episode: 971 Total reward: 11.0 Training loss: 3.5215 Explore P: 0.0103\n",
      "Episode: 972 Total reward: 8.0 Training loss: 4.8291 Explore P: 0.0103\n",
      "Episode: 973 Total reward: 12.0 Training loss: 3.9976 Explore P: 0.0103\n",
      "Episode: 974 Total reward: 11.0 Training loss: 646.0111 Explore P: 0.0103\n",
      "Episode: 975 Total reward: 9.0 Training loss: 3.5782 Explore P: 0.0103\n",
      "Episode: 976 Total reward: 10.0 Training loss: 3.2867 Explore P: 0.0103\n",
      "Episode: 977 Total reward: 9.0 Training loss: 2.6895 Explore P: 0.0103\n",
      "Episode: 978 Total reward: 11.0 Training loss: 1.7033 Explore P: 0.0103\n",
      "Episode: 979 Total reward: 8.0 Training loss: 3.9854 Explore P: 0.0103\n",
      "Episode: 980 Total reward: 9.0 Training loss: 509.2502 Explore P: 0.0103\n",
      "Episode: 981 Total reward: 9.0 Training loss: 4.5026 Explore P: 0.0103\n",
      "Episode: 982 Total reward: 9.0 Training loss: 3.3100 Explore P: 0.0103\n",
      "Episode: 983 Total reward: 12.0 Training loss: 4.0476 Explore P: 0.0103\n",
      "Episode: 984 Total reward: 8.0 Training loss: 2.6884 Explore P: 0.0103\n",
      "Episode: 985 Total reward: 8.0 Training loss: 620.4792 Explore P: 0.0103\n",
      "Episode: 986 Total reward: 10.0 Training loss: 2.4114 Explore P: 0.0103\n",
      "Episode: 987 Total reward: 9.0 Training loss: 2.4386 Explore P: 0.0103\n",
      "Episode: 988 Total reward: 8.0 Training loss: 3.1335 Explore P: 0.0103\n",
      "Episode: 989 Total reward: 8.0 Training loss: 1.9257 Explore P: 0.0103\n",
      "Episode: 990 Total reward: 9.0 Training loss: 1.8175 Explore P: 0.0103\n",
      "Episode: 991 Total reward: 9.0 Training loss: 430.3696 Explore P: 0.0103\n",
      "Episode: 992 Total reward: 9.0 Training loss: 1.8824 Explore P: 0.0103\n",
      "Episode: 993 Total reward: 10.0 Training loss: 2.4980 Explore P: 0.0103\n",
      "Episode: 994 Total reward: 7.0 Training loss: 1.4089 Explore P: 0.0103\n",
      "Episode: 995 Total reward: 9.0 Training loss: 3.4477 Explore P: 0.0103\n",
      "Episode: 996 Total reward: 8.0 Training loss: 1103.3083 Explore P: 0.0103\n",
      "Episode: 997 Total reward: 12.0 Training loss: 759.8696 Explore P: 0.0103\n",
      "Episode: 998 Total reward: 7.0 Training loss: 3.1025 Explore P: 0.0103\n",
      "Episode: 999 Total reward: 11.0 Training loss: 2.2068 Explore P: 0.0103\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "        \n",
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole-them.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env_name, agent):\n",
    "    env = gym.make(env_name)\n",
    "    try:\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, \"checkpoints/cartpole-them.ckpt\")\n",
    "            state = env.reset()\n",
    "            action = env.action_space.sample()\n",
    "            env.render()\n",
    "            state, reward, done, info = env.step(action)\n",
    "            env.render()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            total_reward += reward\n",
    "            while not done:\n",
    "                feed = {agent.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(agent.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                print(state, action, reward)\n",
    "                env.render()\n",
    "            env.close()\n",
    "            print(total_reward)\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/cartpole.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key main/beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-9d23205b3d1b>\", line 34, in <module>\n    saver = tf.train.Saver()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1311, in __init__\n    self.build()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1320, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1357, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 809, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 448, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 860, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1458, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key main/beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key main/beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f84791798f1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-882337aa9e05>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env_name, agent)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoints/cartpole.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1775\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key main/beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-9d23205b3d1b>\", line 34, in <module>\n    saver = tf.train.Saver()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1311, in __init__\n    self.build()\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1320, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1357, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 809, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 448, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 860, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1458, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/Users/uniyomi/omi/miniconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key main/beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "play('CartPole-v1', mainQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(range(16)).reshape((4, 4))\n",
    "x[1] *= 0\n",
    "np.zeros(x.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(values, n_values:int):\n",
    "    return np.eye(n_values)[values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = one_hot(np.array([[2, 1], [2, 3]]), 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0] = 1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
